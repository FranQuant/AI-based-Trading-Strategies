{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b034d73c-4b68-4a31-9ba2-dff5248ecedb",
   "metadata": {},
   "source": [
    "# AI-Based Trading Strategies\n",
    "## Transformer-based Trading Strategy\n",
    "\n",
    "An AI-driven trading framework leveraging a Transformer encoder architecture \n",
    "to forecast market dynamics and backtest systematic long/short strategies on SPY.\n",
    "\n",
    "## Setup & Data Prep\n",
    "### Import Libraries & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d2567-2f5e-4cf2-b1fd-2269b1e67bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os, random, warnings\n",
    "from pathlib import Path; from dataclasses import dataclass\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib.dates as mdates\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, LayerNormalization, Dropout  # ⬅️ CHANGE\n",
    "from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D  # ⬅️ NEW for Transformer\n",
    "from tensorflow.keras.models import Model\n",
    "try:\n",
    "    from tensorflow.keras.optimizers.legacy import Adam\n",
    "except:\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# --- Globals ---\n",
    "OUTPUT_DIR = Path(\"../outputs/05_TRANSFORMER\"); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # ⬅️ CHANGE: new folder\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "# --- Params (Transformer)---\n",
    "BASE_TH       = 0.40        # from Stage B/C winner\n",
    "BAND          = 0.05        # symmetric band for long/short\n",
    "PRED_THRESHOLD = BASE_TH\n",
    "THRESH_BAND    = BAND\n",
    "LONG_THRESHOLD, SHORT_THRESHOLD = round(BASE_TH + BAND, 4), round(BASE_TH - BAND, 4)\n",
    "\n",
    "# Sequence length & split\n",
    "SEQ_LEN   = 28\n",
    "H_SPLIT   = \"2022-01-01\"\n",
    "\n",
    "print(f\"Thresholds -> SHORT={SHORT_THRESHOLD:.2f}, LONG={LONG_THRESHOLD:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c018eed-75b2-4993-b3fb-32eca24f058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Dataset Helper ---\n",
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, parse_dates=[\"Date\"], index_col=\"Date\").sort_index()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    numeric_cols = [\"open\", \"high\", \"low\", \"close\", \"adjusted_close\", \"volume\"]\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return df.ffill()\n",
    "\n",
    "# --- Load Local Dataset ---\n",
    "data_path = Path(\"../data/GSPC_fixed.csv\")\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {data_path.resolve()}\")\n",
    "\n",
    "df = load_data(data_path)\n",
    "print(f\"Data loaded: {df.index.min().date()} → {df.index.max().date()} | {df.shape[0]} rows\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd902c-698d-42be-9460-776c763d21af",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "- Plot **S&P 500 adjusted closing prices** over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eec4e5-9389-4cc4-bc64-ce508fe287d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df[\"adjusted_close\"], linewidth=1.8, color='navy')\n",
    "\n",
    "plt.title(\"S&P 500 Closing Prices (2014 - 2025)\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Closing Price (USD)\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator(1))  \n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ca451-64ac-4599-8df4-2c2d57f7e4e1",
   "metadata": {},
   "source": [
    "## Feature Engineering  \n",
    "### Technical Indicators  \n",
    "\n",
    "Models **learn temporal patterns from sequences**, so we select indicators that enhance **trend detection, volatility awareness, momentum, and volume confirmation**.  \n",
    "These features enrich the input data, giving the model information about **trend strength, volatility regimes, and return distributions**.  \n",
    "\n",
    "---\n",
    "\n",
    "| **Category**              | **Indicator**                           | **Best Periods**                  | **Why Useful?**                                                                 |\n",
    "|---------------------------|-----------------------------------------|-----------------------------------|---------------------------------------------------------------------------------|\n",
    "| **Return Feature**        | Log Returns                             | 1-day                             | Captures raw daily price dynamics; foundation for return-based modeling         |\n",
    "| **Volatility Indicator**  | Rolling Std. of Returns                 | Vol(10)                           | Tracks short-term realized volatility; detects turbulence                       |\n",
    "| **Volatility Indicator**  | Annualized Volatility                   | Vol(10d, 21d, 63d)                | Multi-horizon risk awareness; aligns with trading horizons                      |\n",
    "| **Volatility Indicator**  | Volatility Ratio                        | vol_10d / vol_63d                 | Compares short- vs. long-term volatility to detect regime shifts                |\n",
    "| **Momentum Indicator**    | RSI (Relative Strength Index)           | RSI(14)                           | Detects momentum extremes and overbought/oversold conditions                    |\n",
    "| **Momentum Indicator**    | MACD (Line, Signal, Histogram)          | MACD(12,26,9)                     | Highlights momentum shifts and potential trend reversals                        |\n",
    "| **Volume Indicator**      | OBV (On-Balance Volume)                 | Default (cumulative)              | Confirms price moves with volume flow                                           |\n",
    "| **Volume Indicator**      | Volume Z-Score                          | zscore(20)                        | Standardizes recent volume vs. historical baseline                              |\n",
    "| **Volatility/ATR**        | ATR (Average True Range)                | ATR(14)                           | Captures intraday volatility and trading range                                  |\n",
    "| **Return Horizons**       | Multi-Horizon Returns                   | ret_3d, ret_5d, ret_10d           | Provides forward-looking context for trends                                     |\n",
    "| **Return Distributions**  | Skew & Kurtosis of Returns              | 21d window                        | Captures higher-moment features of return distributions                         |\n",
    "| **Lagged Features**       | Lagged Log Returns                      | lag_1 … lag_7 (7 lags)            | Provide temporal context; lets LSTM exploit autocorrelation                     |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f6b23-9390-4e60-a66d-f0cac728695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Feature Engineering (medium set, skew/kurt kept)\n",
    "# ===========================================\n",
    "\n",
    "# Compute log returns\n",
    "df[\"log_returns\"] = np.log(df[\"adjusted_close\"] / df[\"adjusted_close\"].shift(1))\n",
    "\n",
    "def ensure_log_returns(df):\n",
    "    if \"log_returns\" not in df:\n",
    "        df[\"log_returns\"] = np.log(df[\"adjusted_close\"]).diff().fillna(0.0)\n",
    "    return df\n",
    "\n",
    "def ema(s, span):\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def add_technical_features(df, lags=7):\n",
    "    df = df.copy()\n",
    "    px = df[\"adjusted_close\"]\n",
    "\n",
    "    # Volatility (10d, unannualized)\n",
    "    df[\"volatility_10\"] = (\n",
    "        df[\"log_returns\"].rolling(10).std()\n",
    "        .fillna(df[\"log_returns\"].expanding().std())\n",
    "    )\n",
    "\n",
    "    # RSI(14)\n",
    "    delta = px.diff()\n",
    "    up, down = delta.clip(lower=0), -delta.clip(upper=0)\n",
    "    roll_up = up.ewm(alpha=1/14, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1/14, adjust=False).mean()\n",
    "    rs = (roll_up / (roll_down.replace(0, np.nan))).fillna(0)\n",
    "    df[\"RSI_14\"] = (100 - (100 / (1 + rs))).fillna(50)\n",
    "\n",
    "    # MACD (12,26,9)\n",
    "    ema12, ema26 = ema(px, 12), ema(px, 26)\n",
    "    macd_line = ema12 - ema26\n",
    "    macd_signal = ema(macd_line, 9)\n",
    "    df[\"MACD\"] = (macd_line - macd_signal).fillna(0)\n",
    "    df[\"MACD_line\"] = macd_line.fillna(0)\n",
    "    df[\"MACD_signal\"] = macd_signal.fillna(0)\n",
    "\n",
    "    # OBV\n",
    "    if \"volume\" in df:\n",
    "        direction = np.sign(px.diff().fillna(0))\n",
    "        df[\"OBV\"] = (direction * df[\"volume\"]).fillna(0).cumsum()\n",
    "    else:\n",
    "        df[\"OBV\"] = 0.0\n",
    "\n",
    "    # Lagged log returns\n",
    "    for lag in range(1, lags + 1):\n",
    "        df[f\"lag_{lag}\"] = df[\"log_returns\"].shift(lag)\n",
    "\n",
    "    # Multi-horizon returns\n",
    "    df[\"ret_3d\"]  = np.log(px / px.shift(3))\n",
    "    df[\"ret_5d\"]  = np.log(px / px.shift(5))\n",
    "    df[\"ret_10d\"] = np.log(px / px.shift(10))\n",
    "\n",
    "    # Annualized vol\n",
    "    df[\"vol_10d\"] = df[\"log_returns\"].rolling(10).std() * np.sqrt(252)\n",
    "    df[\"vol_21d\"] = df[\"log_returns\"].rolling(21).std() * np.sqrt(252)\n",
    "    df[\"vol_63d\"] = df[\"log_returns\"].rolling(63).std() * np.sqrt(252)\n",
    "\n",
    "    # ATR(14)\n",
    "    if {\"high\", \"low\"}.issubset(df.columns):\n",
    "        high_low   = df[\"high\"] - df[\"low\"]\n",
    "        high_close = (df[\"high\"] - px.shift()).abs()\n",
    "        low_close  = (df[\"low\"]  - px.shift()).abs()\n",
    "        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "        df[\"ATR_14\"] = tr.rolling(14).mean()\n",
    "    else:\n",
    "        df[\"ATR_14\"] = np.nan\n",
    "\n",
    "    # Skew/Kurtosis over 21d\n",
    "    df[\"ret_skew_21\"] = df[\"log_returns\"].rolling(21).skew()\n",
    "    df[\"ret_kurt_21\"] = df[\"log_returns\"].rolling(21).kurt()\n",
    "\n",
    "    # Volume zscore\n",
    "    if \"volume\" in df:\n",
    "        vol = df[\"volume\"]\n",
    "        df[\"vol_zscore_20\"] = (vol - vol.rolling(20).mean()) / (vol.rolling(20).std() + 1e-12)\n",
    "    else:\n",
    "        df[\"vol_zscore_20\"] = 0.0\n",
    "\n",
    "    # Vol ratio\n",
    "    df[\"vol_ratio\"] = df[\"vol_10d\"] / (df[\"vol_63d\"] + 1e-12)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Build / clean df ---\n",
    "df = ensure_log_returns(df)\n",
    "df = add_technical_features(df, lags=7)\n",
    "\n",
    "# Final feature set\n",
    "FEATURE_COLS = [\n",
    "    \"log_returns\", \"volatility_10\", \"RSI_14\", \"MACD\", \"OBV\",\n",
    "    \"MACD_line\", \"MACD_signal\",\n",
    "    \"ret_3d\", \"ret_5d\", \"ret_10d\",\n",
    "    \"vol_10d\", \"vol_21d\", \"vol_63d\", \"vol_ratio\",\n",
    "    \"ATR_14\",\n",
    "    \"ret_skew_21\", \"ret_kurt_21\",\n",
    "    \"vol_zscore_20\",\n",
    "] + [f\"lag_{i}\" for i in range(1, 8)]\n",
    "\n",
    "# Target\n",
    "if \"label\" not in df:\n",
    "    df[\"label\"] = (df[\"log_returns\"].shift(-1) > 0).astype(int)\n",
    "\n",
    "df = df.dropna(subset=FEATURE_COLS + [\"label\"]).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434b4b7-ab81-4389-9c3d-88d60d614660",
   "metadata": {},
   "source": [
    "## Sequence builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b05942-82bc-4e6d-ad0a-3b622c0131bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X2d, y1d, seq_length=SEQ_LEN):\n",
    "    \"\"\"\n",
    "    Build rolling sequences of length `seq_length`.\n",
    "    Returns X_seq, y_seq.\n",
    "    \"\"\"\n",
    "    Xv = X2d.values if hasattr(X2d, \"values\") else np.asarray(X2d)\n",
    "    yv = y1d.values if hasattr(y1d, \"values\") else np.asarray(y1d)\n",
    "    assert len(Xv) == len(yv)\n",
    "    if len(Xv) < seq_length:\n",
    "        raise ValueError(\"Not enough rows for sequence length.\")\n",
    "\n",
    "    # X windows: [i … i+seq_len-1]\n",
    "    X_seq = np.stack([Xv[i:i+seq_length] for i in range(len(Xv) - seq_length + 1)], axis=0)\n",
    "\n",
    "    # y aligned with last timestep of each window (index i+seq_len-1)\n",
    "    y_seq = np.array([yv[i+seq_length-1] for i in range(len(Xv) - seq_length + 1)])\n",
    "\n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4624e4-06cb-42c8-88a9-db3b1cecb8e8",
   "metadata": {},
   "source": [
    "# Model Implementation & Training\n",
    "## Model Definition & Initialization\n",
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9831b-fa41-4574-9fb7-bd0bbf62951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Transformer Encoder Model for Binary Classification\n",
    "# ====================================================\n",
    "def get_transformer_model(input_shape,\n",
    "                          embed_dim=64, num_heads=4, ff_dim=128,\n",
    "                          dropout=0.30, lr=1e-4):\n",
    "    \"\"\"Builds a simple Transformer encoder for binary classification.\"\"\"\n",
    "    inputs = Input(shape=input_shape, name=\"inputs\")\n",
    "\n",
    "    # --- Linear projection of features (acts as embedding) ---\n",
    "    x = Dense(embed_dim, name=\"proj\")(inputs)\n",
    "\n",
    "    # --- Multi-Head Attention ---\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads,\n",
    "                                     key_dim=embed_dim,\n",
    "                                     name=\"mha\")(x, x)\n",
    "    x = LayerNormalization(name=\"ln_attn\")(x + attn_output)   # residual + norm\n",
    "\n",
    "    # --- Feed-Forward Network (FFN) ---\n",
    "    ffn = Dense(ff_dim, activation=\"relu\", name=\"ffn_1\")(x)\n",
    "    ffn = Dense(embed_dim, name=\"ffn_2\")(ffn)\n",
    "    x = LayerNormalization(name=\"ln_ffn\")(x + ffn)            # residual + norm\n",
    "\n",
    "    # --- Global pooling across timesteps ---\n",
    "    x = GlobalAveragePooling1D(name=\"gap\")(x)\n",
    "    x = Dropout(dropout, name=\"dropout\")(x)\n",
    "\n",
    "    # --- Output layer ---\n",
    "    outputs = Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "    # --- Build & compile ---\n",
    "    model = Model(inputs, outputs, name=\"transformer_encoder\")\n",
    "    model.compile(optimizer=Adam(learning_rate=lr),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69ab9a-37d0-4277-befc-b030d4a7bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build Transformer model ---\n",
    "model = get_transformer_model((SEQ_LEN, len(FEATURE_COLS)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2377063-c28c-471b-a527-00c7e3fe36bb",
   "metadata": {},
   "source": [
    "## Cross-Validation Diagnostics (Stages A–G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7dd3f6-bcd2-4c84-9ff4-60d4dff86ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cross-Validation Diagnostics (Stages A–G)\n",
    "# ===========================================\n",
    "tscv = TimeSeriesSplit(n_splits=6, test_size=252)\n",
    "\n",
    "X_base = df[FEATURE_COLS]\n",
    "y_all  = df[\"label\"]\n",
    "\n",
    "preds_cv, labels_cv, idx_cv = [], [], []\n",
    "fold_rows = []\n",
    "last_scaler = None\n",
    "\n",
    "for k, (tr_pos, va_pos) in enumerate(tscv.split(X_base)):\n",
    "    tr_idx = X_base.index[tr_pos]; va_idx = X_base.index[va_pos]\n",
    "    \n",
    "    # --- Features (train/val, no leakage) [removed regime features]\n",
    "    Xtr_2d = df.loc[tr_idx, FEATURE_COLS].values\n",
    "    Xva_2d = df.loc[va_idx, FEATURE_COLS].values\n",
    "\n",
    "\n",
    "    # --- Scaling\n",
    "    sc = StandardScaler().fit(Xtr_2d)\n",
    "    Xtr = sc.transform(Xtr_2d); Xva = sc.transform(Xva_2d)\n",
    "\n",
    "    # --- Targets\n",
    "    ytr = y_all.loc[tr_idx]; yva = y_all.loc[va_idx]\n",
    "\n",
    "    # --- Sequence construction\n",
    "    Xtr_seq, ytr_seq = create_sequences(Xtr, ytr, SEQ_LEN)\n",
    "    Xva_seq, yva_seq = create_sequences(Xva, yva, SEQ_LEN)\n",
    "    va_seq_idx = va_idx[SEQ_LEN-1:]\n",
    "\n",
    "    # --- Model init + training\n",
    "    model = get_transformer_model((SEQ_LEN, Xtr_seq.shape[2]))\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0)\n",
    "    rl = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=0)\n",
    "\n",
    "    model.fit(\n",
    "        Xtr_seq, ytr_seq,\n",
    "        validation_data=(Xva_seq, yva_seq),\n",
    "        epochs=20, batch_size=64,\n",
    "        verbose=0,\n",
    "        callbacks=[es, rl]\n",
    "    )\n",
    "\n",
    "    assert model.get_layer(\"proj\").units == 64, \"Transformer embed_dim not 64 — re-run the model def cell.\"\n",
    "    \n",
    "    # --- Predictions\n",
    "    pv = model.predict(Xva_seq, verbose=0).flatten()\n",
    "    preds_cv.extend(pv)\n",
    "    labels_cv.extend(yva_seq)\n",
    "    idx_cv.extend(va_seq_idx)\n",
    "\n",
    "    # --- Metrics\n",
    "    yhat = (pv >= PRED_THRESHOLD).astype(int)\n",
    "    fold_rows.append({\n",
    "        \"fold\": k+1,\n",
    "        \"accuracy\":  accuracy_score(yva_seq, yhat),\n",
    "        \"precision\": precision_score(yva_seq, yhat, zero_division=0),\n",
    "        \"recall\":    recall_score(yva_seq, yhat, zero_division=0),\n",
    "        \"f1\":        f1_score(yva_seq, yhat, zero_division=0),\n",
    "        \"roc_auc\":   roc_auc_score(yva_seq, pv),\n",
    "        \"pr_auc\":    average_precision_score(yva_seq, pv),\n",
    "    })\n",
    "    last_scaler = sc\n",
    "\n",
    "# --- Collect CV results\n",
    "preds_cv = np.array(preds_cv)\n",
    "labels_cv = np.array(labels_cv)\n",
    "idx_cv = pd.DatetimeIndex(idx_cv)\n",
    "\n",
    "df_folds = pd.DataFrame(fold_rows)\n",
    "display(df_folds)\n",
    "print(\"Mean (fold):\")\n",
    "display(df_folds.mean(numeric_only=True))\n",
    "\n",
    "# --- Persist diagnostics for Stage A/F sweeps \n",
    "np.save(OUTPUT_DIR/\"cv_predictions.npy\", preds_cv)\n",
    "np.save(OUTPUT_DIR/\"cv_labels.npy\", labels_cv)\n",
    "np.save(OUTPUT_DIR/\"cv_index.npy\", idx_cv.values.astype(\"datetime64[ns]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5409b979-6d81-4ccd-a520-f8127e4a974e",
   "metadata": {},
   "source": [
    "### Backtesting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae7f4c-a84e-45c7-96f9-ce66a95aae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Backtesting Module (Config + Engine) \n",
    "# ====================================================\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BTConfig:\n",
    "    # Thresholds\n",
    "    long_threshold: float = 0.55\n",
    "    short_threshold: float = 0.45\n",
    "    neutral_band: bool = True   # do nothing in (short, long)\n",
    "\n",
    "    # Position management\n",
    "    min_hold_days: int = 1\n",
    "    stop_daily: float = -0.02\n",
    "    # stop_trailing: float = -0.08\n",
    "    roll_loss_window: int = 10\n",
    "    roll_loss_thresh: float = -0.05\n",
    "\n",
    "    # Vol targeting\n",
    "    use_vol_targeting: bool = True\n",
    "    vol_window: int = 15\n",
    "    target_annual_vol: float = 0.12\n",
    "    max_leverage: float = 2.0\n",
    "    vol_floor: float = 1e-8\n",
    "\n",
    "    # Costs\n",
    "    cost_bps_per_turnover: float = 5.0  # 5 bps per turnover event\n",
    "\n",
    "\n",
    "def run_backtest_clean(\n",
    "    df: pd.DataFrame,\n",
    "    predictions: pd.Series | np.ndarray,\n",
    "    pred_index: pd.DatetimeIndex,\n",
    "    cfg: BTConfig = None,\n",
    "    price_col: str = \"adjusted_close\",\n",
    "    ret_col: str = \"log_returns\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Leakage-safe backtest:\n",
    "      - Align strictly to pred_index window\n",
    "      - Signals from predictions (neutral band optional)\n",
    "      - Enforce min_hold_days\n",
    "      - Apply stops (daily, trailing, rolling loss)\n",
    "      - Simple vol targeting\n",
    "      - Transaction costs on position changes\n",
    "      - Turnover & hit_rate computed\n",
    "    \"\"\"\n",
    "    assert ret_col in df.columns, f\"{ret_col} missing from df\"\n",
    "\n",
    "    cfg = cfg or BTConfig()\n",
    "\n",
    "    # ---- Align strictly on pred_index ----\n",
    "    win_mask = df.index.isin(pred_index)\n",
    "    out = df.loc[win_mask, [ret_col, price_col]].copy()\n",
    "    preds = pd.Series(predictions, index=pred_index).loc[out.index]\n",
    "    out[\"p\"] = preds\n",
    "\n",
    "    # ---- Raw signal from thresholds ----\n",
    "    if cfg.neutral_band:\n",
    "        sig = np.select(\n",
    "            [out[\"p\"] >= cfg.long_threshold, out[\"p\"] <= cfg.short_threshold],\n",
    "            [1, -1],\n",
    "            default=0\n",
    "        )\n",
    "    else:\n",
    "        sig = np.where(out[\"p\"] >= cfg.long_threshold, 1, -1)\n",
    "\n",
    "    # ---- Enforce min_hold_days + stops ----\n",
    "    pos = np.zeros(len(sig), dtype=float)\n",
    "    hold = 0\n",
    "    rolling_pnl = []\n",
    "\n",
    "    for i in range(1, len(sig)):\n",
    "        pos[i] = pos[i-1]\n",
    "        hold = hold + 1 if pos[i] != 0 else 0\n",
    "        pnl = pos[i-1] * out[ret_col].iloc[i]\n",
    "\n",
    "        if pos[i-1] != 0:\n",
    "            rolling_pnl.append(pnl)\n",
    "            if len(rolling_pnl) > cfg.roll_loss_window:\n",
    "                rolling_pnl.pop(0)\n",
    "\n",
    "            # --- Daily stop ---\n",
    "            if pnl <= cfg.stop_daily:\n",
    "                pos[i] = 0; hold = 0; rolling_pnl.clear()\n",
    "            # --- Rolling loss stop ---\n",
    "            elif sum(rolling_pnl) <= cfg.roll_loss_thresh:\n",
    "                pos[i] = 0; hold = 0; rolling_pnl.clear()\n",
    "\n",
    "        # --- Entry / hold enforcement ---\n",
    "        if pos[i-1] == 0:\n",
    "            pos[i] = sig[i]\n",
    "            hold = 1 if pos[i] != 0 else 0\n",
    "            rolling_pnl.clear()\n",
    "        elif hold < cfg.min_hold_days:\n",
    "            pos[i] = pos[i-1]\n",
    "        elif sig[i] != pos[i-1]:\n",
    "            pos[i] = sig[i]\n",
    "            hold = 1 if pos[i] != 0 else 0\n",
    "            rolling_pnl.clear()\n",
    "\n",
    "    out[\"pos_raw\"] = pos\n",
    "\n",
    "    # ---- Vol targeting ----\n",
    "    if cfg.use_vol_targeting:\n",
    "        roll_vol = out[ret_col].rolling(cfg.vol_window).std().clip(lower=cfg.vol_floor)\n",
    "        target_daily = cfg.target_annual_vol / np.sqrt(252.0)\n",
    "        lever = (target_daily / roll_vol).clip(upper=cfg.max_leverage)\n",
    "        out[\"pos\"] = out[\"pos_raw\"] * lever.fillna(0.0)\n",
    "    else:\n",
    "        out[\"pos\"] = out[\"pos_raw\"]\n",
    "\n",
    "    # ---- Costs ----\n",
    "    pos_change = out[\"pos\"].diff().fillna(0.0)\n",
    "    turnover_events = (pos_change != 0).astype(int)\n",
    "    cost = (cfg.cost_bps_per_turnover / 1e4) * np.abs(pos_change)\n",
    "    out[\"cost\"] = cost\n",
    "\n",
    "    # ---- Strategy & BH returns ----\n",
    "    out[\"strat_log_ret_gross\"] = out[\"pos\"].shift(1).fillna(0.0) * out[ret_col]\n",
    "    out[\"strat_log_cost\"] = np.log1p(-out[\"cost\"].clip(upper=0.999999)).fillna(0.0)\n",
    "    out[\"strat_log_ret\"] = out[\"strat_log_ret_gross\"] + out[\"strat_log_cost\"]\n",
    "    out[\"bh_log_ret\"] = out[ret_col]\n",
    "\n",
    "    # ---- Equity curves ----\n",
    "    out[\"strategy_equity\"] = np.exp(out[\"strat_log_ret\"].cumsum())\n",
    "    out[\"bh_equity\"] = np.exp(out[\"bh_log_ret\"].cumsum())\n",
    "\n",
    "    # ---- Metrics ----\n",
    "    def ann_mu(x): return x.mean() * 252\n",
    "    def ann_vol(x): return x.std(ddof=0) * np.sqrt(252)\n",
    "    def sharpe(x):  return ann_mu(x) / (ann_vol(x) + 1e-12)\n",
    "    def max_dd(e):  return (e / e.cummax() - 1).min()\n",
    "\n",
    "    strat_log = out[\"strat_log_ret\"].dropna()\n",
    "    bh_log    = out[\"bh_log_ret\"].dropna()\n",
    "\n",
    "    summary = {\n",
    "        \"window_start\": str(out.index[0].date()),\n",
    "        \"window_end\":   str(out.index[-1].date()),\n",
    "        \"bars\":         int(len(out)),\n",
    "        \"total_return\": float(np.exp(strat_log.sum()) - 1),\n",
    "        \"bh_total_return\": float(np.exp(bh_log.sum()) - 1),\n",
    "        \"cagr\":        float(np.exp(ann_mu(strat_log)) - 1),\n",
    "        \"bh_cagr\":     float(np.exp(ann_mu(bh_log)) - 1),\n",
    "        \"ann_vol\":     float(ann_vol(strat_log)),\n",
    "        \"bh_ann_vol\":  float(ann_vol(bh_log)),\n",
    "        \"sharpe\":      float(sharpe(strat_log)),\n",
    "        \"bh_sharpe\":   float(sharpe(bh_log)),\n",
    "        \"max_drawdown\":    float(max_dd(out[\"strategy_equity\"])),\n",
    "        \"bh_max_drawdown\": float(max_dd(out[\"bh_equity\"])),\n",
    "        \"hit_rate\":    float((strat_log > 0).mean()),\n",
    "        \"turnover\":    int(turnover_events.sum()),\n",
    "    }\n",
    "    return out, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e4057-57e1-46c5-bfdf-064390f3163d",
   "metadata": {},
   "source": [
    "### Stage A – Threshold sweep (diagnostic only)\n",
    "Stage A sweeps a band of prediction thresholds to understand how Sharpe and related metrics respond without changing the global deployment setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5575ed5c-13ec-4e97-ab2e-a95ebc8b3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Stage A: Sharpe vs Threshold (CV diagnostics)\n",
    "# ====================================================\n",
    "\n",
    "try:\n",
    "    BTConfig\n",
    "    run_backtest_clean\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Backtest engine (BTConfig/run_backtest_clean) not found in this notebook.\")\n",
    "\n",
    "# --- Align CV predictions and index ---\n",
    "if len(preds_cv) != len(idx_cv):\n",
    "    print(f\"⚠️ Mismatch detected: preds={len(preds_cv)}, idx={len(idx_cv)}\")\n",
    "    n = min(len(preds_cv), len(idx_cv))\n",
    "    preds_cv = preds_cv[:n]\n",
    "    idx_cv   = idx_cv[:n]\n",
    "\n",
    "pred_series_cv = pd.Series(preds_cv, index=idx_cv, name=\"p\")\n",
    "\n",
    "# --- Threshold sweep (adaptive around PRED_THRESHOLD) ---\n",
    "th_grid = np.round(\n",
    "    np.arange(PRED_THRESHOLD - 0.05, PRED_THRESHOLD + 0.051, 0.01),\n",
    "    3\n",
    ")\n",
    "# Keep thresholds in [0,1]\n",
    "th_grid = th_grid[(th_grid > 0) & (th_grid < 1)]\n",
    "\n",
    "rows = []\n",
    "for th in th_grid:\n",
    "    cfg = BTConfig(\n",
    "        long_threshold=min(th + 0.05, 0.99),\n",
    "        short_threshold=max(th - 0.05, 0.01)\n",
    "    )\n",
    "    _, stats = run_backtest_clean(df, pred_series_cv, pred_index=idx_cv, cfg=cfg)\n",
    "    rows.append({\n",
    "        \"threshold\": th,\n",
    "        \"sharpe\": stats[\"sharpe\"],\n",
    "        \"cagr\": stats[\"cagr\"],\n",
    "        \"hit_rate\": stats[\"hit_rate\"]\n",
    "    })\n",
    "\n",
    "df_sweep = pd.DataFrame(rows).sort_values(\"sharpe\", ascending=False).reset_index(drop=True)\n",
    "display(df_sweep)\n",
    "\n",
    "# --- Plot diagnostics (scatter only) ---\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(df_sweep[\"threshold\"], df_sweep[\"sharpe\"],\n",
    "            s=70, color=\"tab:blue\", edgecolor=\"k\", alpha=0.85)\n",
    "plt.axhline(0, color=\"k\", ls=\"--\", lw=1)\n",
    "plt.title(\"Stage A: Sharpe vs Threshold (CV diagnostics)\")\n",
    "plt.xlabel(\"Base threshold\")\n",
    "plt.ylabel(\"Sharpe\")\n",
    "plt.grid(True, ls=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Save results ---\n",
    "df_sweep.to_csv(OUTPUT_DIR/\"threshold_sweep_cv.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2266ae1f-4017-42c9-b124-036771560928",
   "metadata": {},
   "source": [
    "## Stage B – Confirmatory Backtests\n",
    "Stage B reruns the backtest at fixed thresholds to validate the Stage A sweep and capture full diagnostics for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31a93e-02cf-4951-a0c4-eb816e1268c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Stage B – Confirmatory Backtests (CV-based)\n",
    "# ============================================\n",
    "import contextlib, io\n",
    "\n",
    "TUNING_DIR = OUTPUT_DIR / 'tuning'\n",
    "stage_b_root = TUNING_DIR / 'stageB'\n",
    "stage_b_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Stage B thresholds: center around PRED_THRESHOLD with ±0.02 margin\n",
    "stage_b_thresholds = [\n",
    "    round(PRED_THRESHOLD, 3),\n",
    "    round(PRED_THRESHOLD + 0.01, 3),\n",
    "    round(PRED_THRESHOLD - 0.01, 3),\n",
    "]\n",
    "# Filter to keep thresholds in (0,1)\n",
    "stage_b_thresholds = [th for th in stage_b_thresholds if 0 < th < 1]\n",
    "\n",
    "stage_b_records = []\n",
    "\n",
    "\n",
    "def _save_stage_b_returns_plot(df_bt, out_dir):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(df_bt.index, df_bt['log_returns'], label='Actual Returns', alpha=0.45)\n",
    "    ax.plot(df_bt.index, df_bt['strat_log_ret'], label='Strategy Log Returns', alpha=0.45)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', linewidth=0.8)\n",
    "    ax.set_title('Actual Returns vs Strategy Log Returns')\n",
    "    ax.set_ylabel('Daily Log Returns')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_dir / 'returns_comparison.png', dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_stage_b_equity_plot(df_bt, out_dir):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(df_bt.index, df_bt['strategy_equity'], label='Strategy Equity Curve', color='blue')\n",
    "    ax.plot(df_bt.index, df_bt['bh_equity'], label='Buy & Hold Benchmark', color='gray', linestyle='--')\n",
    "    pred_start = df_bt['p'].first_valid_index()\n",
    "    if pred_start is not None:\n",
    "        ax.axvline(pred_start, color='black', linestyle=':', linewidth=0.8, label='Prediction Start')\n",
    "    ax.set_title('Equity Curve Comparison')\n",
    "    ax.set_ylabel('Cumulative Value (Rebased)')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_dir / 'equity_curve_strategy.png', dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def _save_stage_b_relative_plot(df_bt, out_dir):\n",
    "    rel = df_bt['strategy_equity'] / df_bt['bh_equity']\n",
    "    pred_start = df_bt['p'].first_valid_index()\n",
    "    if pred_start is not None:\n",
    "        base_value = rel.loc[pred_start]\n",
    "        if np.isfinite(base_value) and base_value != 0:\n",
    "            rel = rel / base_value\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(df_bt.index, rel, label='Strategy vs Benchmark (rebased)', color='green', alpha=0.85)\n",
    "    ax.axhline(y=1.0, color='black', linestyle=':', linewidth=0.8)\n",
    "    ax.set_title('Relative Performance vs Benchmark')\n",
    "    ax.set_ylabel('Relative Ratio (Rebased to 1.0)')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_dir / 'relative_performance.png', dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Run confirmatory tests ---\n",
    "for base_th in stage_b_thresholds:\n",
    "    long_th = round(base_th + 0.05, 4)\n",
    "    short_th = round(base_th - 0.05, 4)\n",
    "    cfg = BTConfig(long_threshold=long_th, short_threshold=short_th)\n",
    "\n",
    "    stage_dir = stage_b_root / f'th_{base_th:.2f}'\n",
    "    stage_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        df_bt, bt_summary = run_backtest_clean(\n",
    "            df=df,\n",
    "            predictions=pd.Series(preds_cv, index=idx_cv, name=\"p\"),\n",
    "            pred_index=idx_cv,\n",
    "            cfg=cfg,\n",
    "        )\n",
    "\n",
    "    _save_stage_b_returns_plot(df_bt, stage_dir)\n",
    "    _save_stage_b_equity_plot(df_bt, stage_dir)\n",
    "    _save_stage_b_relative_plot(df_bt, stage_dir)\n",
    "\n",
    "    stage_b_records.append({\n",
    "        'threshold': float(base_th),\n",
    "        'long_threshold': float(long_th),\n",
    "        'short_threshold': float(short_th),\n",
    "        'sharpe': float(bt_summary.get('sharpe', np.nan)),\n",
    "        'cagr': float(bt_summary.get('cagr', np.nan)),\n",
    "        'max_drawdown': float(bt_summary.get('max_drawdown', np.nan)),\n",
    "        'turnover': float(bt_summary.get('turnover', np.nan)),\n",
    "        'hit_rate': float(bt_summary.get('hit_rate', np.nan)),\n",
    "    })\n",
    "\n",
    "stage_b_df = pd.DataFrame(stage_b_records)\n",
    "stage_b_csv = TUNING_DIR / 'stageB_results.csv'\n",
    "stage_b_df.to_csv(stage_b_csv, index=False)\n",
    "\n",
    "comparison_cols = ['threshold', 'sharpe', 'cagr', 'max_drawdown', 'turnover', 'hit_rate']\n",
    "formatters = {\n",
    "    'threshold': lambda x: f\"{x:.2f}\",\n",
    "    'sharpe': lambda x: f\"{x:.3f}\",\n",
    "    'cagr': lambda x: f\"{x:.3f}\",\n",
    "    'max_drawdown': lambda x: f\"{x:.3f}\",\n",
    "    'turnover': lambda x: f\"{x:.2f}\",\n",
    "    'hit_rate': lambda x: f\"{x:.3f}\",\n",
    "}\n",
    "\n",
    "stage_b_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e547e-0f31-4670-89e3-6589ce1e374c",
   "metadata": {},
   "source": [
    "### Stage C – Risk / BTConfig grid (fast sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24249b-3f77-4e99-b61d-b25ccb622925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Stage C – Risk / BTConfig grid (fast sweep, CV)\n",
    "# ============================================\n",
    "from itertools import product\n",
    "import contextlib, io\n",
    "\n",
    "TUNE_DIR   = OUTPUT_DIR / \"tuning\"\n",
    "STAGEC_DIR = TUNE_DIR / \"stageC_risk\"\n",
    "STAGEC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use CV predictions directly\n",
    "predictions = pd.Series(preds_cv, index=idx_cv, name=\"p\")\n",
    "pred_index  = idx_cv\n",
    "\n",
    "# --- Stage C thresholds (use global Params) ---\n",
    "base_kwargs = dict(\n",
    "    long_threshold = LONG_THRESHOLD,\n",
    "    short_threshold= SHORT_THRESHOLD,\n",
    "    neutral_band   = True,\n",
    ")\n",
    "\n",
    "print(f\"Stage C using global thresholds: SHORT={SHORT_THRESHOLD:.2f}, LONG={LONG_THRESHOLD:.2f}\")\n",
    "\n",
    "# Grid search\n",
    "grid = {\n",
    "    \"min_hold_days\":    [1, 3, 5],\n",
    "    \"use_vol_targeting\":[True],\n",
    "    \"target_annual_vol\":[0.08, 0.10, 0.12],\n",
    "    \"max_leverage\":     [1.0, 1.5],\n",
    "    \"stop_daily\":       [-0.010, -0.015],\n",
    "    \"roll_loss_window\": [10, 12],\n",
    "    \"roll_loss_thresh\": [-0.030, -0.040],\n",
    "}\n",
    "\n",
    "keys = list(grid.keys())\n",
    "records = []\n",
    "\n",
    "for vals in product(*[grid[k] for k in keys]):\n",
    "    kw = dict(zip(keys, vals))\n",
    "    cfg = BTConfig(**{**base_kwargs, **kw})\n",
    "    with contextlib.redirect_stdout(io.StringIO()):  # silence prints\n",
    "        df_bt, summary = run_backtest_clean(\n",
    "            df=df,\n",
    "            predictions=predictions,\n",
    "            pred_index=pred_index,\n",
    "            cfg=cfg,\n",
    "        )\n",
    "    row = {**kw}\n",
    "    row.update({m: summary.get(m) for m in\n",
    "                [\"sharpe\",\"cagr\",\"max_drawdown\",\"ann_vol\",\"turnover\",\"hit_rate\"]})\n",
    "    records.append(row)\n",
    "\n",
    "df_stageC = pd.DataFrame(records)\n",
    "df_stageC = df_stageC.sort_values([\"sharpe\",\"cagr\"], ascending=[False, False]).reset_index(drop=True)\n",
    "display(df_stageC.head(10))\n",
    "\n",
    "out_csv = STAGEC_DIR / \"risk_grid.csv\"\n",
    "df_stageC.to_csv(out_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e0a30-8a40-473e-807e-77c77273a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "pivot = df_stageC.pivot_table(values=\"sharpe\", index=\"min_hold_days\", columns=\"target_annual_vol\")\n",
    "sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"Sharpe by Hold Days vs Target Volatility\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796caf8b-a873-416f-9899-52c66da3f6b7",
   "metadata": {},
   "source": [
    "### Stage D: Distribution of Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b43394-45b3-4669-aa37-2bf34bdfaa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Stage D – Distribution of Model Predictions\n",
    "# ============================================\n",
    "print(pd.Series(preds_cv).describe())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "# --- (1) Histogram with thresholds (param-driven) ---\n",
    "axes[0].hist(predictions, bins=30, color=\"navy\", alpha=0.7)\n",
    "axes[0].axvline(SHORT_THRESHOLD, color=\"red\", linestyle=\"--\", label=f\"Short ≤ {SHORT_THRESHOLD:.2f}\")\n",
    "axes[0].axvline(LONG_THRESHOLD, color=\"green\", linestyle=\"--\", label=f\"Long ≥ {LONG_THRESHOLD:.2f}\")\n",
    "axes[0].axvspan(SHORT_THRESHOLD, LONG_THRESHOLD, color=\"gray\", alpha=0.2, label=\"Neutral Zone\")\n",
    "axes[0].set_title(\"Distribution (Histogram)\")\n",
    "axes[0].set_xlabel(\"Predicted Probability\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].legend()\n",
    "\n",
    "# --- (2) KDE plot with quartile markers ---\n",
    "sns.histplot(predictions, bins=30, kde=True, color=\"navy\", alpha=0.6, ax=axes[1])\n",
    "q25, q50, q75 = np.percentile(predictions, [25, 50, 75])\n",
    "for q, label in zip([q25, q50, q75], [\"25%\", \"Median\", \"75%\"]):\n",
    "    axes[1].axvline(q, color=\"orange\", linestyle=\":\", linewidth=1.5, label=label)\n",
    "axes[1].set_title(\"Distribution (KDE + Quartiles)\")\n",
    "axes[1].set_xlabel(\"Predicted Probability\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eebd6d-1341-45d3-9dbf-2dddc93d9e40",
   "metadata": {},
   "source": [
    "### Stage E — CV Threshold Tuning & Diagnostics\n",
    "- Narrow Threshold Sweep (0.49–0.51)\n",
    "- Threshold Diagnostics (Trades + Hit Rate, CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e9fb2-82fd-4e81-b121-cf6ac5361d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Stage E.1: Narrow Threshold Sweep (around PRED_THRESHOLD, CV preds only)\n",
    "# ====================================================\n",
    "\n",
    "# --- Force Stage E to use CV only (no OOS leakage) ---\n",
    "preds_stageE = pd.Series(preds_cv, index=idx_cv, name=\"p\")\n",
    "pred_idx_stageE = idx_cv\n",
    "\n",
    "# --- Prepare df_test from CV predictions ---\n",
    "df_test = df.loc[pred_idx_stageE].copy()\n",
    "df_test[\"p\"] = preds_stageE\n",
    "preds = preds_stageE.values  # numpy array for backtest\n",
    "\n",
    "# --- Define sweep range (±0.01 around PRED_THRESHOLD) ---\n",
    "base_thresholds = np.round(\n",
    "    np.arange(PRED_THRESHOLD - 0.01, PRED_THRESHOLD + 0.0101, 0.002),\n",
    "    3\n",
    ")\n",
    "results = []\n",
    "\n",
    "for th in base_thresholds:\n",
    "    long_th = min(th + 0.005, 0.99)\n",
    "    short_th = max(th - 0.005, 0.01)\n",
    "    cfg = BTConfig(long_threshold=long_th, short_threshold=short_th, neutral_band=True)\n",
    "\n",
    "    _, stats = run_backtest_clean(df_test, preds, pred_idx_stageE, cfg=cfg)\n",
    "    stats[\"base_threshold\"] = th\n",
    "    results.append(stats)\n",
    "\n",
    "# --- Collect results ---\n",
    "df_thresh = pd.DataFrame(results)\n",
    "df_thresh.to_csv(OUTPUT_DIR/\"tuning\"/\"stageE1_threshold_sweep.csv\", index=False)\n",
    "\n",
    "# --- Plot Sharpe vs threshold ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_thresh[\"base_threshold\"], df_thresh[\"sharpe\"], marker=\"o\")\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\", lw=1)\n",
    "plt.title(\"Stage E.1: Sharpe vs Threshold (CV Narrow Band)\")\n",
    "plt.xlabel(\"Base Threshold\")\n",
    "plt.ylabel(\"Sharpe Ratio\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# Stage E.2: Threshold Diagnostics (Trades + Hit Rate, CV)\n",
    "# =========================================================\n",
    "trade_counts = []\n",
    "hit_rates = []\n",
    "\n",
    "for th in base_thresholds:\n",
    "    cfg = BTConfig(\n",
    "        long_threshold=min(th + 0.005, 0.99),\n",
    "        short_threshold=max(th - 0.005, 0.01),\n",
    "        neutral_band=True\n",
    "    )\n",
    "    _, stats = run_backtest_clean(df_test, preds, pred_idx_stageE, cfg=cfg)\n",
    "    trade_counts.append(stats.get('turnover', 0))\n",
    "    hit_rates.append(stats.get('hit_rate', np.nan))\n",
    "\n",
    "# --- Collect diagnostics ---\n",
    "diagnostics_df = pd.DataFrame({\n",
    "    \"threshold\": base_thresholds,\n",
    "    \"trade_count\": trade_counts,\n",
    "    \"hit_rate\": hit_rates\n",
    "})\n",
    "diagnostics_df.to_csv(OUTPUT_DIR/\"tuning\"/\"stageE2_threshold_diagnostics.csv\", index=False)\n",
    "\n",
    "# --- Plot diagnostics ---\n",
    "fig, ax1 = plt.subplots(figsize=(8,5))\n",
    "ax1.set_title(\"Stage E.2: Threshold Diagnostics — Trades & Hit Rate (CV)\")\n",
    "ax1.plot(diagnostics_df[\"threshold\"], diagnostics_df[\"trade_count\"], \n",
    "         marker=\"o\", color=\"tab:blue\", label=\"Trade Count\")\n",
    "ax1.set_xlabel(\"Base Threshold\")\n",
    "ax1.set_ylabel(\"Trade Count\", color=\"tab:blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(diagnostics_df[\"threshold\"], diagnostics_df[\"hit_rate\"], \n",
    "         marker=\"s\", color=\"tab:green\", label=\"Hit Rate\")\n",
    "ax2.set_ylabel(\"Hit Rate\", color=\"tab:green\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a2350-2421-45fd-a8b4-592ed6c70f85",
   "metadata": {},
   "source": [
    "# AI Strategy Performance Review\n",
    "## OOS Confirmatory Run (2022+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db42b7-132f-4d74-9af5-dafad3088a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# OOS confirmatory (train+val < 2022, predict 2022+)\n",
    "# ======================================================\n",
    "\n",
    "print(\"=== Stage H: OOS confirmatory (train+val < 2022, predict 2022+) ===\")\n",
    "\n",
    "# --- Define split points ---\n",
    "VAL_SPLIT = \"2020-01-01\"   # start of validation\n",
    "H_SPLIT   = \"2022-01-01\"   # OOS cutoff\n",
    "EVAL_END   = \"2025-03-07\" \n",
    "\n",
    "# --- Index sets ---\n",
    "trainval_idx = df.index[df.index < H_SPLIT]   # all data < 2022\n",
    "test_idx     = df.index[df.index >= H_SPLIT] # OOS data ≥ 2022\n",
    "\n",
    "# --- Features (train+val, test) [no regime features] ---\n",
    "Xtrval2 = df.loc[trainval_idx, FEATURE_COLS].values\n",
    "Xte2    = df.loc[test_idx,     FEATURE_COLS].values\n",
    "\n",
    "ytrval  = df.loc[trainval_idx, \"label\"]\n",
    "yte     = df.loc[test_idx,     \"label\"]\n",
    "\n",
    "# --- Scaling (fit on train+val only) ---\n",
    "sc = StandardScaler().fit(Xtrval2)\n",
    "Xtrval = sc.transform(Xtrval2)\n",
    "Xte    = sc.transform(Xte2)\n",
    "\n",
    "# --- Sequences ---\n",
    "Xtrval_seq, ytrval_seq = create_sequences(Xtrval, ytrval, SEQ_LEN)\n",
    "Xte_seq,    yte_seq    = create_sequences(Xte,    yte,    SEQ_LEN)\n",
    "pred_index_oos = pd.DatetimeIndex(df.loc[test_idx].index[SEQ_LEN-1:])  # fixed alignment\n",
    "\n",
    "# --- Model ---\n",
    "model = get_transformer_model(\n",
    "    (SEQ_LEN, Xtrval_seq.shape[2]),\n",
    "    embed_dim=64, num_heads=4, ff_dim=128,\n",
    "    dropout=0.30, lr=4.24e-05\n",
    ")\n",
    "\n",
    "assert model.get_layer(\"proj\").units == 64, \"Transformer embed_dim not 64 — re-run the model def cell.\"\n",
    "\n",
    "# Train once on full train+val (<2022), no validation callbacks now\n",
    "hist = model.fit(\n",
    "    Xtrval_seq, ytrval_seq,\n",
    "    epochs=20, batch_size=64, verbose=0\n",
    ")\n",
    "\n",
    "# --- Predict OOS ---\n",
    "pred_oos = model.predict(Xte_seq, verbose=0).flatten()\n",
    "pred_series_oos = pd.Series(pred_oos, index=pred_index_oos, name=\"p\")\n",
    "\n",
    "# --- Backtest ---\n",
    "cfg = BTConfig(long_threshold=LONG_THRESHOLD, short_threshold=SHORT_THRESHOLD)\n",
    "df_bt_oos, summary_oos = run_backtest_clean(\n",
    "    df=df, predictions=pred_series_oos, pred_index=pred_index_oos, cfg=cfg\n",
    ")\n",
    "\n",
    "# --- Save artifacts ---\n",
    "oos_dir = OUTPUT_DIR/\"oos_final\"\n",
    "oos_dir.mkdir(parents=True, exist_ok=True)\n",
    "np.save(oos_dir/\"pred_oos.npy\", pred_oos)\n",
    "np.save(oos_dir/\"pred_index_oos.npy\", pred_index_oos.values.astype(\"datetime64[ns]\"))\n",
    "pd.DataFrame([summary_oos]).to_csv(oos_dir/\"backtest_oos_summary.csv\", index=False)\n",
    "\n",
    "# --- Plot equity curve ---\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.plot(df_bt_oos.loc[pred_index_oos].index,\n",
    "         df_bt_oos.loc[pred_index_oos, \"strategy_equity\"],\n",
    "         label=\"Strategy (OOS)\", color=\"blue\")\n",
    "plt.plot(df_bt_oos.loc[pred_index_oos].index,\n",
    "         df_bt_oos.loc[pred_index_oos, \"bh_equity\"],\n",
    "         label=\"Buy & Hold\", color=\"green\")\n",
    "\n",
    "pred_start = pred_index_oos[0]\n",
    "plt.axvline(pred_start, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "plt.text(pred_start,\n",
    "         df_bt_oos[\"bh_equity\"].max() * 0.9,\n",
    "         f\"Prediction Start: {pred_start.date()}\",\n",
    "         rotation=0, color=\"red\", ha=\"left\", va=\"top\", fontsize=10,\n",
    "         bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"red\"))\n",
    "\n",
    "plt.title(\"Equity Curve — OOS (2022+)\", fontsize=14)\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Equity (Rebased)\")\n",
    "plt.legend(); plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(oos_dir/\"equity_curve_oos_enhanced.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# --- Pretty print summary (metadata + comparison) ---\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Metadata\n",
    "meta_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Window Start\", \"Window End\", \"Bars\"],\n",
    "    \"Value\": [\n",
    "        summary_oos[\"window_start\"],\n",
    "        summary_oos[\"window_end\"],\n",
    "        summary_oos[\"bars\"],\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Strategy vs Benchmark metrics\n",
    "metrics_map = {\n",
    "    \"Total Return\": (\"total_return\", \"bh_total_return\"),\n",
    "    \"CAGR\": (\"cagr\", \"bh_cagr\"),\n",
    "    \"Ann Vol\": (\"ann_vol\", \"bh_ann_vol\"),\n",
    "    \"Sharpe\": (\"sharpe\", \"bh_sharpe\"),\n",
    "    \"Max Drawdown\": (\"max_drawdown\", \"bh_max_drawdown\"),\n",
    "    \"Hit Rate\": (\"hit_rate\", None),\n",
    "    \"Turnover\": (\"turnover\", None),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for label, (s_key, b_key) in metrics_map.items():\n",
    "    rows.append([\n",
    "        label,\n",
    "        f\"{summary_oos.get(s_key, np.nan):.3f}\",\n",
    "        f\"{summary_oos.get(b_key, np.nan):.3f}\" if b_key else \"\"\n",
    "    ])\n",
    "\n",
    "summary_df = pd.DataFrame(rows, columns=[\"Metric\", \"Strategy\", \"Benchmark\"])\n",
    "\n",
    "print(\"=== OOS Backtest Summary ===\")\n",
    "print(tabulate(meta_df, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "print(tabulate(summary_df, headers=\"keys\", tablefmt=\"github\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e41867-8c6c-435d-8478-8fd2212c7c3d",
   "metadata": {},
   "source": [
    "### Actual Returns vs Predicted Returns\n",
    "Shows how well the signals align with actual movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac560b52-ecb8-42d4-82ce-37ea2cd84363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Plot: Actual vs Strategy Returns (Full history, OOS marked)\n",
    "# ======================================================\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_bt_oos.index, df_bt_oos['log_returns'], \n",
    "         label=\"Actual Returns\", alpha=0.45)\n",
    "plt.plot(df_bt_oos.index, df_bt_oos['strat_log_ret'], \n",
    "         label=\"Strategy Log Returns\", alpha=0.45)\n",
    "\n",
    "# OOS cutoff marker\n",
    "#plt.axvline(pd.to_datetime(\"2022-01-01\"), color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "#plt.text(pd.to_datetime(\"2022-01-01\"),\n",
    "         #df_bt_oos['log_returns'].max() * 0.8,\n",
    "         #\"OOS Start\",\n",
    "         #rotation=0, color=\"red\", ha=\"left\", va=\"top\", fontsize=10,\n",
    "         #bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"red\"))\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.title(\"Actual Returns vs Strategy Log Returns (Full, OOS Marked)\")\n",
    "plt.ylabel(\"Daily Log Returns\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'returns_comparison_full_oos.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306663e5-f653-4acb-a8fa-9a83403fc709",
   "metadata": {},
   "source": [
    "## `quantstats` Analytics Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d12f6-fae6-4194-a23d-bc61bbdabd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Stage H: QuantStats Analysis (OOS Backtest 2022+)\n",
    "# ===================================================\n",
    "import quantstats as qs\n",
    "\n",
    "# Convert log returns to simple daily returns\n",
    "strategy_simple_returns = np.exp(df_bt_oos['strat_log_ret']) - 1\n",
    "benchmark_simple_returns = np.exp(df_bt_oos['bh_log_ret']) - 1\n",
    "\n",
    "# Generate a Performance Report\n",
    "qs.reports.basic(strategy_simple_returns, benchmark=benchmark_simple_returns)\n",
    "\n",
    "# Cumulative Returns Comparison (Strategy vs. Benchmark)\n",
    "qs.plots.returns(strategy_simple_returns, benchmark=benchmark_simple_returns)\n",
    "\n",
    "# Drawdown Analysis\n",
    "qs.plots.drawdown(strategy_simple_returns)\n",
    "\n",
    "# Rolling Sharpe Ratio\n",
    "qs.plots.rolling_sharpe(strategy_simple_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b466cf-0313-4874-ab3f-aac7cfd79c6b",
   "metadata": {},
   "source": [
    "## OOS Stress Test: Why 2022+?\n",
    "\n",
    "We deliberately select **2022 onwards** as the out-of-sample (OOS) evaluation window.  \n",
    "This period represents one of the most challenging market regimes in recent history:\n",
    "\n",
    "- **Rate Shock:** The Fed executed its fastest tightening cycle since the Volcker era, driving rates from near-zero to 4.5%+ within a year.  \n",
    "- **Correlation Breakdown:** Both equities and Treasuries sold off together, breaking the classic 60/40 diversification.  \n",
    "- **Volatility Spikes:** Equity (VIX), bond (MOVE), and FX volatilities surged simultaneously.  \n",
    "- **Liquidity Stress:** Quantitative tightening (QT) drained market liquidity and widened credit spreads.  \n",
    "- **Macro Narrative Shifts:** Inflation shock → policy shock → earnings compression.\n",
    "\n",
    "By validating the Transformer on this window, we **stress test robustness under adverse macro conditions** rather than just capturing momentum in easy bull markets.  \n",
    "Even modest Sharpe ratios in this regime are meaningful, since most strategies struggled to adapt.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
