{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4bdeb6-ebc0-4c67-8c82-bc7599b49a93",
   "metadata": {},
   "source": [
    "# AI-Based Trading Strategies\n",
    "\n",
    "**Author:** J. Francisco Salazar ©\n",
    "\n",
    "**Date:** 2025-02-15\n",
    "\n",
    "## LSTM with Attention based Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f633af-b493-4856-8dcf-6b7c0810955f",
   "metadata": {},
   "source": [
    "# End-to-End Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23477c08-be2d-4372-a16a-b29a2b1bc11d",
   "metadata": {},
   "source": [
    "## Model Configuration & Hyperparameters\n",
    "\n",
    "> Set your Attention-LSTM parameters here before running the notebook.  \n",
    "> All model architecture, training, and input shape are controlled here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e475c-6eae-4543-8052-83a5b30a0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== CONFIG BLOCK ==========================\n",
    "# ⚠️ Fill in your preferred values before running the notebook.\n",
    "\n",
    "LSTM_UNITS = None        # e.g., 80\n",
    "DROPOUT_RATE = None      # e.g., 0.1\n",
    "\n",
    "LEARNING_RATE = None     # e.g., 0.0000262\n",
    "EPOCHS = None            # e.g., 50\n",
    "BATCH_SIZE = None        # e.g., 128\n",
    "\n",
    "SEQUENCE_LENGTH = None   # e.g., 7\n",
    "N_FEATURES = None        # e.g., 15\n",
    "\n",
    "# ================================================================\n",
    "\n",
    "# Optional: fail-fast check\n",
    "assert all(v is not None for v in [\n",
    "    LSTM_UNITS, DROPOUT_RATE,\n",
    "    LEARNING_RATE, EPOCHS, BATCH_SIZE,\n",
    "    SEQUENCE_LENGTH, N_FEATURES\n",
    "]), \"⚠️ Please fill in all config values above before running.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7181e18-1826-41c0-9a76-5d50222d0df3",
   "metadata": {},
   "source": [
    "### Import Libraries & Dependencies\n",
    "We import the necessary libraries for:\n",
    "- API connection (`requests`)\n",
    "- Data handling (`pandas`, `numpy`)\n",
    "- Logging (`logging`)\n",
    "- Visualization (`matplotlib`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd08fb0-407f-4bbb-884c-cdea1d90d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "\n",
    "# === Reproducibility Settings ===\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Optional: Force deterministic behavior (if needed for presentation)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d1cc8-c32d-4c9a-bd4a-f1b49382c9cd",
   "metadata": {},
   "source": [
    "## API Configuration\n",
    "- Load the API Key from environment variables.\n",
    "- Define the ticker (`GSPC.INDX` for S&P 500).\n",
    "- Set the date range (`2015-2025`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787235b6-678f-475d-b972-424c9c6788a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- API Configuration ---\n",
    "API_KEY = os.getenv(\"EODHD_API_KEY\")  # Load API Key securely\n",
    "TICKER = \"GSPC.INDX\"\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2025-01-01\"\n",
    "BASE_URL = \"https://eodhd.com/api/eod/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5232a0-65c8-4dc5-8657-18cdee717735",
   "metadata": {},
   "source": [
    "## API Request Function \n",
    "- Fetches historical data using the **EODHD API**.\n",
    "- Implements **error handling & retry logic** to manage failed API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065bfd21-e5f7-4239-b57b-fcbb90cca519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_eod_data(ticker, api_key, start_date, end_date, retries=3, delay=5):\n",
    "    \"\"\"\n",
    "    Fetches historical market data from EODHD API with retry logic.\n",
    "\n",
    "    Parameters:\n",
    "    - ticker (str): Stock or index ticker symbol\n",
    "    - api_key (str): API authentication token\n",
    "    - start_date (str): Start date for data retrieval\n",
    "    - end_date (str): End date for data retrieval\n",
    "    - retries (int): Number of retry attempts in case of failure\n",
    "    - delay (int): Delay between retries (exponential backoff)\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Market data if successful, else None\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}{ticker}?api_token={api_key}&from={start_date}&to={end_date}&fmt=json\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data:\n",
    "                    logging.info(f\"Successfully retrieved {len(data)} records.\")\n",
    "                    return pd.DataFrame(data)\n",
    "                else:\n",
    "                    logging.warning(\"API returned an empty dataset.\")\n",
    "            else:\n",
    "                logging.error(f\"API request failed with status {response.status_code}: {response.text}\")\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"API request error: {e}\")\n",
    "\n",
    "        sleep(delay * (2 ** attempt))  # Exponential backoff\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef0553-ae40-43ab-bb8f-1807a17313eb",
   "metadata": {},
   "source": [
    "## Fetch & Preprocess Data\n",
    "- Convert `date` column to datetime format.\n",
    "- Set `date` as index for time-series analysis.\n",
    "- Convert numeric columns to `float`.\n",
    "- Handle missing values using **forward-fill**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fcd44-1275-4d36-bab8-16ea8515f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_eod_data(TICKER, API_KEY, START_DATE, END_DATE)\n",
    "\n",
    "if df is not None:\n",
    "    \n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    df.set_index(\"date\", inplace=True)\n",
    "\n",
    "    numeric_cols = [\"open\", \"high\", \"low\", \"close\", \"adjusted_close\", \"volume\"]\n",
    "    df[numeric_cols] = df[numeric_cols].astype(float)\n",
    "\n",
    "    df.ffill(inplace=True)  \n",
    "\n",
    "    display(df.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1517f7-4688-4fa9-9271-82dfbcc6e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Check Data Completeness ---\n",
    "print(\"\\n Data Overview:\")\n",
    "print(df.info())  # Check structure & missing values\n",
    "print(\"\\n Missing Values Summary:\")\n",
    "print(df.isna().sum())  # Count missing values per column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99293e-5e69-4d15-a6ec-5f9d574d9ea9",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "- **Figure 1:** **S&P 500 adjusted closing prices** over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941cb04-b477-497b-8271-4bfe4f3f7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df[\"adjusted_close\"], linewidth=1.8, color='navy')\n",
    "\n",
    "plt.title(\"Figure 1: S&P 500 Closing Prices (2014 - 2025)\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Closing Price (USD)\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator(1))  \n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecfba2-910f-4f40-9442-d88450d5235b",
   "metadata": {},
   "source": [
    "## Feature Engineering & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a06f8-d26c-49a2-807c-85086a01f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log returns\n",
    "df[\"log_returns\"] = np.log(df[\"adjusted_close\"] / df[\"adjusted_close\"].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b2298-8f5b-4c77-afe9-af0a30276270",
   "metadata": {},
   "source": [
    "### Volatility-Adjusted Returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31aa27-7a35-4500-9838-5fa810f92c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Volatility (10-day)\n",
    "df[\"volatility_10\"] = df[\"log_returns\"].rolling(10).std()\n",
    "\n",
    "# Volatility-Adjusted Returns\n",
    "df[\"vol_adj_returns\"] = df[\"log_returns\"] / df[\"volatility_10\"]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee4520-c172-493f-8e4b-42497b5b300d",
   "metadata": {},
   "source": [
    "### Technical Indicators  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc7b8f-b046-4c7c-ba7c-65a66f75e9b4",
   "metadata": {},
   "source": [
    "**Table 1:**\n",
    "\n",
    "LSTM models **learn patterns from sequences**, so we should select indicators that enhance **trend detection, volatility awareness, and momentum signals**.\n",
    "\n",
    "| **Category**              | **Indicator**                                      | **Best Periods** | **Why Useful?**                                   |\n",
    "|---------------------------|---------------------------------------------------|----------------|--------------------------------------------------|\n",
    "| **Moving Averages**        | **EMA (Exponential Moving Avg)**                 | **EMA(50, 200)** | Captures recent trend changes faster           |\n",
    "| **Momentum Indicators**    | **RSI (Relative Strength Index)**                | **RSI(14)** | Identifies momentum & overbought/oversold conditions   |\n",
    "| **Momentum Indicators**    | **MACD (Moving Avg Convergence Divergence)**     | **MACD(12,26,9)** | Detects momentum shifts & trend reversals   |\n",
    "| **Volatility Indicators**  | **ATR (Average True Range)**                     | **ATR(14)** | Measures market volatility & stop-loss adjustments        |\n",
    "| **Trend Indicators**       | **SAR (Parabolic SAR)**                          | **SAR(Default)** | Tracks trend direction & reversals                        |\n",
    "| **Trend Indicators**       | **SLOPE (Slope Indicator)**                      | **SLOPE(14)** | Confirms trend strength                        |\n",
    "| **Trend Indicators**       | **ADX (Average Directional Index)**              | **ADX(14)** | Measures trend strength without direction |\n",
    "| **Volume Indicators**      | **OBV (On-Balance Volume)**                      | **OBV(Default)** | Confirms trend strength via volume              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e7a64-82dc-48d2-8a4f-1efd57a7a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum Indicators: RSI & MACD\n",
    "def compute_rsi(df, column=\"adjusted_close\", period=14):\n",
    "    delta = df[column].diff()\n",
    "    gain = np.where(delta > 0, delta, 0)\n",
    "    loss = np.where(delta < 0, -delta, 0)\n",
    "    avg_gain = np.convolve(gain, np.ones(period)/period, mode='same')\n",
    "    avg_loss = np.convolve(loss, np.ones(period)/period, mode='same')\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(df, column=\"adjusted_close\", short_period=12, long_period=26, signal_period=9):\n",
    "    short_ema = df[column].ewm(span=short_period, adjust=False).mean()\n",
    "    long_ema = df[column].ewm(span=long_period, adjust=False).mean()\n",
    "    macd_line = short_ema - long_ema\n",
    "    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()\n",
    "    return macd_line, signal_line\n",
    "\n",
    "# Moving Averages: EMA 50 & EMA 200\n",
    "def compute_ema(df, column=\"adjusted_close\", period=50):\n",
    "    return df[column].ewm(span=period, adjust=False).mean()\n",
    "\n",
    "def compute_ema_200(df, column=\"adjusted_close\", period=200):\n",
    "    return df[column].ewm(span=period, adjust=False).mean()\n",
    "\n",
    "# Volatility Indicator: ATR\n",
    "def compute_atr(df, period=14):\n",
    "    tr = np.maximum(df[\"high\"] - df[\"low\"],\n",
    "                     np.maximum(abs(df[\"high\"] - df[\"adjusted_close\"].shift()),\n",
    "                                abs(df[\"low\"] - df[\"adjusted_close\"].shift())))\n",
    "    return tr.rolling(window=period).mean()\n",
    "\n",
    "# Trend Indicators: SAR, Slope & ADX\n",
    "def compute_slope(df, column=\"adjusted_close\", period=14):\n",
    "    return df[column].diff(period) / period\n",
    "\n",
    "def compute_sar(df, acceleration=0.02, maximum=0.2):\n",
    "    sar = np.zeros(len(df))\n",
    "    trend = 1  # Assume first trend is bullish\n",
    "    extreme_point = df[\"high\"].iloc[0]\n",
    "    af = acceleration\n",
    "    \n",
    "    for i in range(1, len(df)):\n",
    "        sar[i] = sar[i - 1] + af * (extreme_point - sar[i - 1]) if trend == 1 else sar[i - 1] - af * (sar[i - 1] - extreme_point)\n",
    "        extreme_point = max(extreme_point, df[\"high\"].iloc[i]) if trend == 1 else min(extreme_point, df[\"low\"].iloc[i])\n",
    "        af = min(af + acceleration, maximum)\n",
    "    return sar\n",
    "\n",
    "def compute_adx(df, period=14):\n",
    "    plus_dm = np.maximum(df[\"high\"].diff(), 0)\n",
    "    minus_dm = np.maximum(-df[\"low\"].diff(), 0)\n",
    "    tr = np.maximum(df[\"high\"] - df[\"low\"],\n",
    "                     np.maximum(abs(df[\"high\"] - df[\"adjusted_close\"].shift()),\n",
    "                                abs(df[\"low\"] - df[\"adjusted_close\"].shift())))\n",
    "    plus_di = 100 * (plus_dm.rolling(window=period).mean() / tr.rolling(window=period).mean())\n",
    "    minus_di = 100 * (minus_dm.rolling(window=period).mean() / tr.rolling(window=period).mean())\n",
    "    dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)\n",
    "    return dx.rolling(window=period).mean()\n",
    "\n",
    "# Volume Indicator: OBV\n",
    "def compute_obv_vectorized(df):\n",
    "    direction = np.sign(df[\"adjusted_close\"].diff()).fillna(0)\n",
    "    return (direction * df[\"volume\"]).cumsum()\n",
    "\n",
    "\n",
    "# Compute Indicators and Add to DataFrame\n",
    "df[\"EMA_50\"] = compute_ema(df, period=50)\n",
    "df[\"EMA_200\"] = compute_ema_200(df, period=200)\n",
    "df[\"RSI_14\"] = compute_rsi(df, period=14)\n",
    "df[\"MACD\"], df[\"MACD_Signal\"] = compute_macd(df)\n",
    "df[\"ATR_14\"] = compute_atr(df, period=14)\n",
    "df[\"SAR\"] = compute_sar(df)\n",
    "df[\"SLOPE_14\"] = compute_slope(df, period=14)\n",
    "df[\"ADX_14\"] = compute_adx(df, period=14)\n",
    "df[\"OBV\"] = compute_obv_vectorized(df)\n",
    "\n",
    "# Fill any NaNs from rolling calculations\n",
    "df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f6691-e7f9-4d49-88ba-c675a3ae9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aed7d2-7a6e-40c2-80af-e989ca8bd794",
   "metadata": {},
   "source": [
    "### Adding Lag Features (`lag_1` to `lag_7`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cee6b0-bd3a-4bd0-903f-6f95c8247743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "#  Step 1: Plot ACF & PACF BEFORE adding lag features\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot ACF\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_acf(df[\"log_returns\"].dropna(), lags=10, ax=plt.gca())\n",
    "plt.title(\"Auto-Correlation (ACF)\")\n",
    "\n",
    "# Plot PACF\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_pacf(df[\"log_returns\"].dropna(), lags=10, ax=plt.gca())\n",
    "plt.title(\"Partial Auto-Correlation (PACF)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1_acf_pacf_log_returns.png\", dpi=100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#  Step 2: Select significant lags based on ACF/PACF analysis\n",
    "selected_lags = [1, 3, 5]  # Adjust based on the actual ACF/PACF plot\n",
    "\n",
    "#  Step 3: Generate only the selected lags\n",
    "for lag in selected_lags:\n",
    "    df[f'lag_{lag}'] = df['log_returns'].shift(lag)\n",
    "\n",
    "#  Step 4: Drop NaN values caused by shifting\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#  Step 5: Check DataFrame structure after adding optimized lag features\n",
    "print(\"\\n Updated DataFrame Structure After Selecting Significant Lags:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ac2f5-3a00-4202-b166-7ffb8ec07b9e",
   "metadata": {},
   "source": [
    "### Feature Correlation Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27961ba1-fe84-41b7-924e-ba7b49e376d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Generate a mask to hide the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Dynamic figure sizing\n",
    "n_features = corr_matrix.shape[0]\n",
    "plt.figure(figsize=(n_features * 0.5, n_features * 0.5))  \n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix, mask=mask, annot=False, fmt=\".2f\", cmap=\"coolwarm\", center=0,\n",
    "    linewidths=1, cbar_kws={\"shrink\": 0.75}, square=True\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(\"Feature Correlation Matrix\", fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2_feature_correlation_matrix.png\", dpi=100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# -------------- Feature Selection Based on Correlation --------------\n",
    "# Lower the correlation threshold to catch more redundant features\n",
    "threshold = 0.85  # Previously 0.9, now more aggressive\n",
    "\n",
    "# Identify highly correlated features\n",
    "high_corr_features = set()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            colname = corr_matrix.columns[i]\n",
    "            high_corr_features.add(colname)\n",
    "\n",
    "# Drop highly correlated features\n",
    "df.drop(columns=high_corr_features, inplace=True)\n",
    "\n",
    "# Display the dropped features\n",
    "print(\"\\n Dropped Highly Correlated Features:\")\n",
    "print(f\"{list(high_corr_features) if high_corr_features else 'None'}\")\n",
    "\n",
    "# Show updated DataFrame info\n",
    "print(\"\\n Updated DataFrame Structure After Correlation Filtering:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf0bd2-d156-4c85-85a7-5b9f8d6561b5",
   "metadata": {},
   "source": [
    "### Definition of Target Variable (`d`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78fc21-7a85-4c48-87d1-ec65022cb4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Target Variable (`d`) ---\n",
    "df['d'] = np.where(df['log_returns'].shift(-1) > 0, 1, 0)\n",
    "\n",
    "# Drop only NaN values in 'd' caused by shifting (Last row)\n",
    "df.dropna(subset=['d'], inplace=True)\n",
    "\n",
    "# --- Corrected Feature Selection (X) ---\n",
    "X = df[[col for col in [\n",
    "     'open', 'volume', 'log_returns', 'volatility_10', 'vol_adj_returns',\n",
    "    'RSI_14', 'MACD', 'ATR_14', 'SLOPE_14', 'ADX_14', \n",
    "    'lag_1', 'lag_3', 'lag_5'\n",
    "] if col in df.columns]]  # Only include columns that exist\n",
    "\n",
    "# --- Define Target Variable (y) ---\n",
    "y = df['d']\n",
    "\n",
    "# Verify Structure\n",
    "print(\"Feature Matrix (X) Shape:\", X.shape)\n",
    "print(\"Target Variable (y) Shape:\", y.shape)\n",
    "display(df.tail())\n",
    "\n",
    "print(df['d'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12dcb4f-9ca7-4e7d-a016-b103e2482032",
   "metadata": {},
   "source": [
    "### Feature Selection Using `ExtraTreesClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2347283-f13d-4237-9a16-c36301023611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#  Step 1: Define a chronological cutoff date for train/test split\n",
    "train_cutoff = '2022-01-01'  # Modify if needed\n",
    "\n",
    "df_train = df.loc[:train_cutoff]\n",
    "df_test = df.loc[train_cutoff:]\n",
    "\n",
    "X_train = X.loc[df_train.index]\n",
    "y_train = y.loc[df_train.index]\n",
    "X_test = X.loc[df_test.index]\n",
    "y_test = y.loc[df_test.index]\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "#  Step 2: Fit ExtraTreesClassifier ONLY on training set\n",
    "model_fs = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "model_fs.fit(X_train, y_train)\n",
    "\n",
    "#  Step 3: Extract feature importances\n",
    "feature_importance = pd.Series(\n",
    "    model_fs.feature_importances_,\n",
    "    index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "#  Step 4: Plot feature importances\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    x=feature_importance.values, \n",
    "    y=feature_importance.index, \n",
    "    hue=feature_importance.index,  # Explicitly set hue to y variable\n",
    "    palette=\"viridis\",\n",
    "    dodge=False,\n",
    "    legend=False  # Prevent duplicate legend\n",
    ")\n",
    "plt.title(\"Feature Importance (ExtraTreesClassifier)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"3_feature_importance_extratrees_selected.png\", dpi=100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#  Step 5: Select top N features\n",
    "n_top = min(15, len(feature_importance))  # Ensure it does not exceed available features\n",
    "top_features = feature_importance.index[:n_top]\n",
    "\n",
    "print(f\"\\n Top {n_top} Features (Training Set):\")\n",
    "print(top_features.tolist())\n",
    "\n",
    "#  Step 6: Apply selected features to both train & test sets\n",
    "X_train_selected = X_train[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "print(\"\\n Final Feature Set After Selection:\")\n",
    "print(f\"X_train_selected shape: {X_train_selected.shape}\")\n",
    "print(f\"X_test_selected shape: {X_test_selected.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be687a-498d-4ba1-9493-8e75004988b2",
   "metadata": {},
   "source": [
    "### Preparation of `X` and `y` for LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212d6a8-61db-43e8-8a3a-39ea0efacc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "##############################\n",
    "# Step 1: Standardize Training Data Only\n",
    "##############################\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_selected)  # Fit only on training data\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)  # Use same scaler on test set\n",
    "\n",
    "# Convert back to DataFrame for consistency\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train_selected.index, columns=X_train_selected.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test_selected.index, columns=X_test_selected.columns)\n",
    "\n",
    "print(\"\\n **Standardization Completed:** Data is now scaled.\")\n",
    "\n",
    "##############################\n",
    "# Step 2: Create Sequences for LSTM\n",
    "##############################\n",
    "def create_sequences(X_df, y_series, seq_length=7):\n",
    "    \"\"\"\n",
    "    Generates sequences from a DataFrame (X_df) and aligns with the target (y_series).\n",
    "    Returns (X_seq, y_seq).\n",
    "    \"\"\"\n",
    "    X_values = X_df.values  \n",
    "    y_values = y_series.values  \n",
    "\n",
    "    # For each valid starting index i, create a sequence: [i : i + seq_length]\n",
    "    X_seq = np.array([\n",
    "        X_values[i : i + seq_length] \n",
    "        for i in range(len(X_values) - seq_length)\n",
    "    ])\n",
    "    # The label for that sequence is y at index i + seq_length\n",
    "    y_seq = y_values[seq_length:]  \n",
    "\n",
    "    return X_seq, y_seq\n",
    "\n",
    "sequence_length = 7  # Set sequence length\n",
    "\n",
    "# Safety checks before sequence creation\n",
    "if len(X_train_scaled) < sequence_length:\n",
    "    raise ValueError(\" Not enough training data to generate sequences! Adjust `sequence_length` or dataset size.\")\n",
    "if len(X_test_scaled) < sequence_length:\n",
    "    raise ValueError(\" Not enough test data to generate sequences! Adjust `sequence_length` or dataset size.\")\n",
    "\n",
    "# Generate training & test sequences\n",
    "X_seq_train, y_seq_train = create_sequences(X_train_scaled, y_train, seq_length=sequence_length)\n",
    "X_seq_test, y_seq_test = create_sequences(X_test_scaled, y_test, seq_length=sequence_length)\n",
    "\n",
    "##############################\n",
    "# Step 3: Confirm Shapes\n",
    "##############################\n",
    "print(\"\\n Final Sequence Shapes for LSTM Training:\")\n",
    "print(f\"X_seq_train: {X_seq_train.shape} (Samples={X_seq_train.shape[0]}, Time Steps={sequence_length}, Features={X_seq_train.shape[2]})\")\n",
    "print(f\"y_seq_train: {y_seq_train.shape} (Samples={y_seq_train.shape[0]})\")\n",
    "print(f\"X_seq_test:  {X_seq_test.shape} (Samples={X_seq_test.shape[0]}, Time Steps={sequence_length}, Features={X_seq_test.shape[2]})\")\n",
    "print(f\"y_seq_test:  {y_seq_test.shape} (Samples={y_seq_test.shape[0]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee473f-3350-4fa6-8413-c3dc5fcf8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save Sequences and Index for Attention Extraction Notebook ===\n",
    "np.save(\"../data/X_seq_test.npy\", X_seq_test)\n",
    "np.save(\"../data/test_index.npy\", X_test_scaled.index.values)\n",
    "\n",
    "print(\" X_seq_test and test_index saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0aa8f-9e5b-4057-8d15-4998def92370",
   "metadata": {},
   "source": [
    "# Model Implementation & Training\n",
    "## Model Definition & Initialization\n",
    "\n",
    "#### LSTM with Attention Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb6532-9af5-4a6a-9a4e-3406eedb9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Imports + Environment Setup\n",
    "##############################\n",
    "\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow logs & Metal plugin warnings (for Apple Silicon)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "os.environ['METAL_DEVICE_WRAPPER_SUPPRESS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Multiply\n",
    "from tensorflow.keras.optimizers.legacy import Adam  # ⚠️ Legacy optimizer for M1/M2 Macs\n",
    "\n",
    "##############################\n",
    "# Step 1: Define Attention Layer (Unchanged)\n",
    "##############################\n",
    "def attention_layer(inputs):\n",
    "    \"\"\"\n",
    "    Implements a simple attention mechanism.\n",
    "    Args:\n",
    "        inputs: LSTM output sequence (batch_size, time_steps, units)\n",
    "    Returns:\n",
    "        context_vector: Weighted sum of the LSTM outputs (batch_size, units)\n",
    "        attention_weights: Weights computed for each time step\n",
    "    \"\"\"\n",
    "    attention_scores = Dense(1, activation='tanh', name=\"attention_scores\")(inputs)\n",
    "    attention_scores = tf.squeeze(attention_scores, axis=-1)\n",
    "    attention_weights = tf.nn.softmax(attention_scores, axis=1, name=\"attention_weights\")\n",
    "    context_vector = Multiply(name=\"weighted_sum\")([inputs, tf.expand_dims(attention_weights, axis=-1)])\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1, name=\"context_vector\")\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "##############################\n",
    "# Step 2: Define LSTM Model with Attention (Config-Based)\n",
    "##############################\n",
    "def get_attention_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds an LSTM model with an attention mechanism using CONFIG BLOCK values.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "    lstm_output = LSTM(LSTM_UNITS, return_sequences=True, name=\"lstm_layer\")(inputs)\n",
    "    lstm_output = Dropout(DROPOUT_RATE, name=\"dropout_layer\")(lstm_output)\n",
    "\n",
    "    context_vector, attention_weights = attention_layer(lstm_output)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid', name=\"output_layer\")(context_vector)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"LSTM_Attention_Model\")\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "##############################\n",
    "# Step 3: Instantiate the Model (using CONFIG shape)\n",
    "##############################\n",
    "input_shape = (SEQUENCE_LENGTH, N_FEATURES)\n",
    "attention_model = get_attention_model(input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd4f1d-348b-4e81-a21e-9da78cb6e4e8",
   "metadata": {},
   "source": [
    "## Model Training with Rolling Cross-Validation\n",
    "### Training Implementation\n",
    "\n",
    "* Rolling Time-Series Cross-Validation → Ensures robust evaluation.\n",
    "* Scaling within each fold → Prevents data leakage.\n",
    "* Early Stopping → Stops training if validation loss stops improving.\n",
    "* Standard Metrics → Accuracy, Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf41522-89aa-41c1-bf96-3ec77b770e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====================================================\n",
    "# Time-Series Cross-Validation Setup\n",
    "# ====================================================\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=6, test_size=252)\n",
    "predictions, true_labels = [], []\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_seq_train)):\n",
    "    print(f\"\\nProcessing Fold {fold + 1}/{tscv.n_splits}...\")\n",
    "\n",
    "    # Train/test split for current fold\n",
    "    X_train_fold = X_seq_train[train_index]\n",
    "    X_test_fold = X_seq_train[test_index]\n",
    "    y_train_fold = y_seq_train[train_index]\n",
    "    y_test_fold = y_seq_train[test_index]\n",
    "\n",
    "    # Scaling (fold-by-fold)\n",
    "    nsamples_train, timesteps, n_features = X_train_fold.shape\n",
    "    X_train_fold_2d = X_train_fold.reshape(-1, n_features)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold_scaled = scaler.fit_transform(X_train_fold_2d).reshape(X_train_fold.shape)\n",
    "    X_test_fold_scaled = scaler.transform(X_test_fold.reshape(-1, n_features)).reshape(X_test_fold.shape)\n",
    "\n",
    "    # Build & train model using CONFIG-based function\n",
    "    model = get_attention_model((timesteps, n_features))\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fold_scaled, y_train_fold,\n",
    "        validation_data=(X_test_fold_scaled, y_test_fold),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict(X_test_fold_scaled).flatten()\n",
    "    pred_binary = [1 if p >= 0.5 else 0 for p in pred]\n",
    "\n",
    "    predictions.extend(pred)\n",
    "    true_labels.extend(y_test_fold)\n",
    "\n",
    "    # Fold metrics\n",
    "    fold_metrics.append({\n",
    "        \"fold\": fold + 1,\n",
    "        \"accuracy\": accuracy_score(y_test_fold, pred_binary),\n",
    "        \"precision\": precision_score(y_test_fold, pred_binary),\n",
    "        \"recall\": recall_score(y_test_fold, pred_binary),\n",
    "        \"f1_score\": f1_score(y_test_fold, pred_binary)\n",
    "    })\n",
    "\n",
    "# ====================================================\n",
    "# Save + Display Fold Metrics\n",
    "# ====================================================\n",
    "\n",
    "df_fold_metrics = pd.DataFrame(fold_metrics)\n",
    "df_fold_metrics.to_csv(\"fold_metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nFold Metrics:\")\n",
    "display(df_fold_metrics)\n",
    "\n",
    "# Overall performance\n",
    "predictions_binary = [1 if p >= 0.5 else 0 for p in predictions]\n",
    "print(\"\\nCross-Validated Model Performance (Training Set):\")\n",
    "print(f\"Accuracy: {accuracy_score(true_labels, predictions_binary):.4f}\")\n",
    "print(f\"Precision: {precision_score(true_labels, predictions_binary):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_labels, predictions_binary):.4f}\")\n",
    "print(f\"F1-score: {f1_score(true_labels, predictions_binary):.4f}\")\n",
    "\n",
    "print(\"\\nMean Fold Performance:\")\n",
    "print(df_fold_metrics[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c0b1a-4808-46af-b5cd-cb60cfb63579",
   "metadata": {},
   "source": [
    "#### `Saving & Logging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa82821b-6db2-437e-b7a5-5f2092e909b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil \n",
    "\n",
    "# Save the best model from cross-validation\n",
    "model.save(\"best_att_lstm_model.keras\")\n",
    "\n",
    "# Backup in case of overwriting\n",
    "shutil.copy(\"best_att_lstm_model.keras\", \"best_att_lstm_model_backup.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb71dc3-180d-4009-ac16-2c2e092ce6d2",
   "metadata": {},
   "source": [
    "**Up to this section, we have put together a production-grade pipeline for a state-of-the-art DL-based trading strategy.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cb0fc-e2a7-4288-8d7b-e8092dbc370d",
   "metadata": {},
   "source": [
    "# Trading Signal Generation and Backtesting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ebc137-28fa-406a-beab-1852b39f84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest(\n",
    "    df, predictions, \n",
    "    transaction_cost=0.0001, \n",
    "    long_threshold=0.505, \n",
    "    short_threshold=0.495, \n",
    "    min_hold_days=3,\n",
    "    stop_loss_threshold=-0.025,  # Stop-loss: Exit if daily loss > -2.5%\n",
    "    rolling_loss_threshold=-0.05,  # Exit if rolling 10-day return < -5%\n",
    "    volatility_scaling=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Backtests a trading strategy with:\n",
    "    - Lowered probability thresholds for better trade execution.\n",
    "    - Adaptive holding period (exit when probability confidence weakens).\n",
    "    - Stop-loss mechanism to prevent major drawdowns.\n",
    "    - Volatility-based position scaling.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Historical price data including 'log_returns'.\n",
    "        predictions (array-like): Model output probabilities for the test period.\n",
    "        transaction_cost (float): Cost per trade.\n",
    "        long_threshold (float): Minimum probability to enter a long trade.\n",
    "        short_threshold (float): Maximum probability to enter a short trade.\n",
    "        min_hold_days (int): Minimum days to hold a position once taken.\n",
    "        stop_loss_threshold (float): Stop-loss exit threshold per trade.\n",
    "        rolling_loss_threshold (float): Exit if rolling 10-day loss > threshold.\n",
    "        volatility_scaling (bool): If True, position size scales based on volatility.\n",
    "\n",
    "    Returns:\n",
    "        df_backtest (DataFrame): DataFrame with backtest results including an equity curve.\n",
    "    \"\"\"\n",
    "    df_backtest = df.copy()\n",
    "    \n",
    "    # Align predictions with the last len(predictions) rows of df_backtest\n",
    "    df_backtest['p'] = np.nan\n",
    "    df_backtest.loc[df_backtest.index[-len(predictions):], 'p'] = predictions\n",
    "\n",
    "    # Generate raw trading signals based on adjusted thresholds\n",
    "    df_backtest['raw_signal'] = np.select(\n",
    "        [df_backtest['p'] >= long_threshold, df_backtest['p'] <= short_threshold],\n",
    "        [1, -1], default=0\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # Adaptive Holding Period\n",
    "    # ---------------------------\n",
    "    raw_signals = df_backtest['raw_signal']\n",
    "    final_positions = []\n",
    "    current_position = 0\n",
    "    hold_counter = 0\n",
    "\n",
    "    for i in range(len(raw_signals)):\n",
    "        if hold_counter > 0:\n",
    "            # Continue holding the position\n",
    "            final_positions.append(current_position)\n",
    "            hold_counter -= 1\n",
    "        else:\n",
    "            # Allow new signal entry\n",
    "            if raw_signals.iloc[i] != 0:\n",
    "                current_position = raw_signals.iloc[i]\n",
    "                hold_counter = min_hold_days - 1  # Hold for at least min_hold_days\n",
    "                final_positions.append(current_position)\n",
    "            else:\n",
    "                # Exit if probability confidence weakens\n",
    "                current_position = 0\n",
    "                final_positions.append(0)\n",
    "\n",
    "    df_backtest['po'] = final_positions\n",
    "    df_backtest['po_shifted'] = df_backtest['po'].shift(1).fillna(0)  # Avoid look-ahead bias\n",
    "\n",
    "    # ---------------------------\n",
    "    # Stop-Loss Implementation\n",
    "    # ---------------------------\n",
    "    df_backtest['daily_return'] = df_backtest['po_shifted'] * df_backtest['log_returns']\n",
    "    \n",
    "    # Apply stop-loss: Exit position if loss exceeds stop_loss_threshold\n",
    "    stop_loss_exit = df_backtest['daily_return'] <= stop_loss_threshold\n",
    "    df_backtest.loc[stop_loss_exit, 'po_shifted'] = 0\n",
    "\n",
    "    # Apply rolling 10-day loss exit condition\n",
    "    df_backtest['rolling_loss'] = df_backtest['daily_return'].rolling(10).sum()\n",
    "    df_backtest.loc[df_backtest['rolling_loss'] <= rolling_loss_threshold, 'po_shifted'] = 0\n",
    "\n",
    "    # ---------------------------\n",
    "    # Volatility-Based Position Sizing\n",
    "    # ---------------------------\n",
    "    if volatility_scaling:\n",
    "        df_backtest['volatility_30d'] = df_backtest['log_returns'].rolling(30).std()\n",
    "        avg_volatility = df_backtest['volatility_30d'].mean()\n",
    "        df_backtest['position_size'] = df_backtest['po_shifted'] * (avg_volatility / df_backtest['volatility_30d'])\n",
    "    else:\n",
    "        df_backtest['position_size'] = df_backtest['po_shifted']\n",
    "\n",
    "    # ---------------------------\n",
    "    # Compute Strategy Returns\n",
    "    # ---------------------------\n",
    "    df_backtest['strategy_log_return'] = df_backtest['position_size'] * df_backtest['log_returns']\n",
    "\n",
    "    # ---------------------------\n",
    "    # Apply Transaction Costs\n",
    "    # ---------------------------\n",
    "    df_backtest['trade_cost'] = transaction_cost * df_backtest['po'].diff().abs().fillna(0)\n",
    "    df_backtest['strategy_net_log_return'] = df_backtest['strategy_log_return'] - df_backtest['trade_cost']\n",
    "\n",
    "    # ---------------------------\n",
    "    # Compute Equity Curve\n",
    "    # ---------------------------\n",
    "    df_backtest['equity_curve'] = np.exp(df_backtest['strategy_net_log_return'].cumsum())\n",
    "\n",
    "    return df_backtest\n",
    "\n",
    "#  Run the backtest\n",
    "df_backtest = run_backtest(df, predictions)\n",
    "\n",
    "#  Print results\n",
    "print(df_backtest[['log_returns', 'p', 'po', 'strategy_log_return', 'trade_cost', 'strategy_net_log_return', 'equity_curve']].tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3e6c3-1b8e-45ae-9c1f-fd4ef5b05627",
   "metadata": {},
   "source": [
    "# AI Strategy Performance Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d61f8f-7823-4f6d-a52a-1a97508da3a0",
   "metadata": {},
   "source": [
    "## Plot 1: Actual Returns vs Predicted Returns\n",
    "Shows how well the signals align with actual movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c951b-cbb5-4d34-9261-cd79706750f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_backtest.index, df_backtest['log_returns'], label=\"Actual Returns\", alpha=0.45)\n",
    "plt.plot(df_backtest.index, df_backtest['strategy_log_return'], label=\"Predicted Returns\", alpha=0.45)\n",
    "plt.axhline(y=0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.title(\"Actual Returns vs Predicted Returns\")\n",
    "plt.ylabel(\"Daily Log Returns\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"4_actual_vs_predicted_returns.png\", dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d54134-b172-47f2-9b97-aae40af4648a",
   "metadata": {},
   "source": [
    "## Plot 2: Equity Curve of the Strategy\n",
    "Show the cumulative performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ab33d-de6a-4b2d-8ce1-2a3fad06cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_backtest.index, df_backtest['equity_curve'], label=\"Strategy Equity Curve\", color='blue')\n",
    "plt.axhline(1.0, linestyle='--', color='red', label=\"Baseline\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Returns\")\n",
    "plt.title(\"Trading Strategy Backtest: Equity Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"5_equity_curve_attention_lstm.png\", dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3046c9-c1f8-48a5-a81c-2556258dd714",
   "metadata": {},
   "source": [
    "## Plot 3: Strategy vs. Benchmark Comparison\n",
    "Illustrate whether the strategy outperforms a simple buy-and-hold benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16df42-db06-4610-b0b9-6e6f4f575d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert log returns to cumulative returns for buy-and-hold:\n",
    "df_backtest['buy_and_hold'] = np.exp(df_backtest['log_returns'].cumsum())\n",
    "df_backtest['cumulative_strategy'] = df_backtest['equity_curve']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_backtest.index, df_backtest['buy_and_hold'], label=\"Buy-and-Hold (Benchmark)\", alpha=0.55)\n",
    "plt.plot(df_backtest.index, df_backtest['cumulative_strategy'], label=\"Attention-LSTM Strategy\", alpha=0.55)\n",
    "plt.axhline(y=1, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.title(\"Cumulative Performance: Strategy vs. Benchmark\")\n",
    "plt.ylabel(\"Cumulative Returns\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"6_cumulative_performance_attention_lstm_vs_benchmark.png\", dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099b010-0e33-419d-8613-2cbf3e112669",
   "metadata": {},
   "source": [
    "## `quantstats` Analytics Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6631e-c0f7-411a-b720-0f05d3258080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert net log returns to simple daily returns:\n",
    "strategy_simple_returns = np.exp(df_backtest['strategy_net_log_return']) - 1\n",
    "benchmark_simple_returns = np.exp(df_backtest['log_returns']) - 1\n",
    "\n",
    "import quantstats as qs\n",
    "\n",
    "# Generate a Performance Report\n",
    "qs.reports.basic(strategy_simple_returns, benchmark=benchmark_simple_returns)\n",
    "\n",
    "# Cumulative Returns Comparison (Strategy vs. Benchmark)\n",
    "qs.plots.returns(strategy_simple_returns, benchmark=benchmark_simple_returns)\n",
    "\n",
    "# Drawdown Analysis\n",
    "qs.plots.drawdown(strategy_simple_returns)\n",
    "\n",
    "# Rolling Sharpe Ratio\n",
    "qs.plots.rolling_sharpe(strategy_simple_returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21110c14-55e7-4a69-ae8d-ae81bbc2a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export Clean Attention-LSTM Model Results ===\n",
    "\n",
    "df_att = df_backtest[['log_returns', 'p']].copy()\n",
    "df_att.rename(columns={'p': 'predictions'}, inplace=True)\n",
    "\n",
    "# Drop rows with missing predictions\n",
    "df_att.dropna(subset=['predictions'], inplace=True)\n",
    "\n",
    "# Confirm structure\n",
    "print(df_att.head())\n",
    "print(df_att.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca50514-9491-4712-8856-9b0e3314c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_att.to_csv('df_att.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
