{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4bdeb6-ebc0-4c67-8c82-bc7599b49a93",
   "metadata": {},
   "source": [
    "# AI-Based Trading Strategies\n",
    "\n",
    "**Author:** J. Francisco Salazar\n",
    "\n",
    "**Date:** 2025-02-23\n",
    "\n",
    "## LSTM based Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f633af-b493-4856-8dcf-6b7c0810955f",
   "metadata": {},
   "source": [
    "# End-to-End Data Pipeline\n",
    "\n",
    "### Template Notice (Editable Version)\n",
    "> This notebook is a clean, reusable **template** for building and evaluating a stacked LSTM model on financial time series data.\n",
    ">\n",
    "> All key hyperparameters are defined in a centralized config block at the top of the notebook:\n",
    "> \n",
    "> - Architecture: `LSTM_UNITS_1`, `LSTM_UNITS_2`, `DROPOUT_RATE`  \n",
    "> - Training: `LEARNING_RATE`, `EPOCHS`, `BATCH_SIZE`  \n",
    "> - Input shape: `SEQUENCE_LENGTH`, `N_FEATURES`  \n",
    ">\n",
    "> **To use this notebook:**\n",
    "> - Edit the config block with your own values  \n",
    "> - Then run all cells top-to-bottom  \n",
    "> - Outputs are automatically generated and saved\n",
    "\n",
    "This notebook is part of a modular pipeline. Each model is self-contained and organized for publishing, collaboration, and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ab416-b565-466f-8ff5-f173ab7c01d9",
   "metadata": {},
   "source": [
    "## Model Configuration & Hyperparameters\n",
    "\n",
    "> Set your custom model parameters here before running the notebook.  \n",
    "> This block controls the LSTM architecture and training logic.  \n",
    "> You must fill in values below or the notebook will not run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0cd5b-245b-41e1-8f07-b5b5bef61bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== CONFIG BLOCK ==========================\n",
    "# ⚠️ Fill in your preferred values before running the notebook.\n",
    "\n",
    "LSTM_UNITS_1 = None      # e.g., 128\n",
    "LSTM_UNITS_2 = None      # e.g., 8\n",
    "DROPOUT_RATE = None      # e.g., 0.2\n",
    "\n",
    "LEARNING_RATE = None     # e.g., 0.0005\n",
    "EPOCHS = None            # e.g., 30\n",
    "BATCH_SIZE = None        # e.g., 64\n",
    "\n",
    "SEQUENCE_LENGTH = None   # e.g., 7\n",
    "N_FEATURES = None        # e.g., 15\n",
    "\n",
    "# ================================================================\n",
    "\n",
    "# Optional: fail-fast check\n",
    "assert all(v is not None for v in [\n",
    "    LSTM_UNITS_1, LSTM_UNITS_2, DROPOUT_RATE,\n",
    "    LEARNING_RATE, EPOCHS, BATCH_SIZE,\n",
    "    SEQUENCE_LENGTH, N_FEATURES\n",
    "]), \"⚠️ Please fill in all hyperparameter values above before running.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7181e18-1826-41c0-9a76-5d50222d0df3",
   "metadata": {},
   "source": [
    "### Import Libraries & Dependencies\n",
    "We import the necessary libraries for:\n",
    "- API connection (`requests`)\n",
    "- Data handling (`pandas`, `numpy`)\n",
    "- Logging (`logging`)\n",
    "- Visualization (`matplotlib`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd08fb0-407f-4bbb-884c-cdea1d90d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d1cc8-c32d-4c9a-bd4a-f1b49382c9cd",
   "metadata": {},
   "source": [
    "## API Configuration\n",
    "- Load the API Key from environment variables.\n",
    "- Define the ticker (`GSPC.INDX` for S&P 500).\n",
    "- Set the date range (`2014-2025`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787235b6-678f-475d-b972-424c9c6788a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- API Configuration ---\n",
    "API_KEY = os.getenv(\"EODHD_API_KEY\")  # Load API Key securely\n",
    "TICKER = \"GSPC.INDX\"\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2025-01-01\"\n",
    "BASE_URL = \"https://eodhd.com/api/eod/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5232a0-65c8-4dc5-8657-18cdee717735",
   "metadata": {},
   "source": [
    "## API Request Function \n",
    "- Fetches historical data using the **EODHD API**.\n",
    "- Implements **error handling & retry logic** to manage failed API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065bfd21-e5f7-4239-b57b-fcbb90cca519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_eod_data(ticker, api_key, start_date, end_date, retries=3, delay=5):\n",
    "    \"\"\"\n",
    "    Fetches historical market data from EODHD API with retry logic.\n",
    "\n",
    "    Parameters:\n",
    "    - ticker (str): Stock or index ticker symbol\n",
    "    - api_key (str): API authentication token\n",
    "    - start_date (str): Start date for data retrieval\n",
    "    - end_date (str): End date for data retrieval\n",
    "    - retries (int): Number of retry attempts in case of failure\n",
    "    - delay (int): Delay between retries (exponential backoff)\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Market data if successful, else None\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}{ticker}?api_token={api_key}&from={start_date}&to={end_date}&fmt=json\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data:\n",
    "                    logging.info(f\"Successfully retrieved {len(data)} records.\")\n",
    "                    return pd.DataFrame(data)\n",
    "                else:\n",
    "                    logging.warning(\"API returned an empty dataset.\")\n",
    "            else:\n",
    "                logging.error(f\"API request failed with status {response.status_code}: {response.text}\")\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"API request error: {e}\")\n",
    "\n",
    "        sleep(delay * (2 ** attempt))  # Exponential backoff\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef0553-ae40-43ab-bb8f-1807a17313eb",
   "metadata": {},
   "source": [
    "## Fetch & Preprocess Data\n",
    "- Convert `date` column to datetime format.\n",
    "- Set `date` as index for time-series analysis.\n",
    "- Convert numeric columns to `float`.\n",
    "- Handle missing values using **forward-fill**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fcd44-1275-4d36-bab8-16ea8515f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_eod_data(TICKER, API_KEY, START_DATE, END_DATE)\n",
    "\n",
    "if df is not None:\n",
    "    \n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    df.set_index(\"date\", inplace=True)\n",
    "\n",
    "    numeric_cols = [\"open\", \"high\", \"low\", \"close\", \"adjusted_close\", \"volume\"]\n",
    "    df[numeric_cols] = df[numeric_cols].astype(float)\n",
    "\n",
    "    df.ffill(inplace=True)  \n",
    "\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99293e-5e69-4d15-a6ec-5f9d574d9ea9",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "- Plot **S&P 500 adjusted closing prices** over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941cb04-b477-497b-8271-4bfe4f3f7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df[\"adjusted_close\"], linewidth=1.8, color='navy')\n",
    "\n",
    "plt.title(\"S&P 500 Closing Prices (2014 - 2025)\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Closing Price (USD)\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator(1))  \n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecfba2-910f-4f40-9442-d88450d5235b",
   "metadata": {},
   "source": [
    "## Feature Engineering & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a06f8-d26c-49a2-807c-85086a01f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log returns\n",
    "df[\"log_returns\"] = np.log(df[\"adjusted_close\"] / df[\"adjusted_close\"].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b2298-8f5b-4c77-afe9-af0a30276270",
   "metadata": {},
   "source": [
    "### Volatility-Adjusted Returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31aa27-7a35-4500-9838-5fa810f92c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Volatility (10-day)\n",
    "df[\"volatility_10\"] = df[\"log_returns\"].rolling(10).std()\n",
    "\n",
    "# Volatility-Adjusted Returns\n",
    "df[\"vol_adj_returns\"] = df[\"log_returns\"] / df[\"volatility_10\"]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee4520-c172-493f-8e4b-42497b5b300d",
   "metadata": {},
   "source": [
    "### Technical Indicators  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc7b8f-b046-4c7c-ba7c-65a66f75e9b4",
   "metadata": {},
   "source": [
    "LSTM models **learn patterns from sequences**, so we should select indicators that enhance **trend detection, volatility awareness, and momentum signals**.\n",
    "\n",
    "| **Category**              | **Indicator**                                      | **Best Periods** | **Why Useful?**                                   |\n",
    "|---------------------------|---------------------------------------------------|----------------|--------------------------------------------------|\n",
    "| **Moving Averages**        | **EMA (Exponential Moving Avg)**                 | **EMA(50, 200)** | Captures recent trend changes faster           |\n",
    "| **Momentum Indicators**    | **RSI (Relative Strength Index)**                | **RSI(14)** | Identifies momentum & overbought/oversold conditions   |\n",
    "| **Momentum Indicators**    | **MACD (Moving Avg Convergence Divergence)**     | **MACD(12,26,9)** | Detects momentum shifts & trend reversals   |\n",
    "| **Volatility Indicators**  | **ATR (Average True Range)**                     | **ATR(14)** | Measures market volatility & stop-loss adjustments        |\n",
    "| **Trend Indicators**       | **SAR (Parabolic SAR)**                          | **SAR(Default)** | Tracks trend direction & reversals                        |\n",
    "| **Trend Indicators**       | **SLOPE (Slope Indicator)**                      | **SLOPE(14)** | Confirms trend strength                        |\n",
    "| **Trend Indicators**       | **ADX (Average Directional Index)**              | **ADX(14)** | Measures trend strength without direction |\n",
    "| **Volume Indicators**      | **OBV (On-Balance Volume)**                      | **OBV(Default)** | Confirms trend strength via volume              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e7a64-82dc-48d2-8a4f-1efd57a7a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum Indicators: RSI & MACD\n",
    "def compute_rsi(df, column=\"adjusted_close\", period=14):\n",
    "    delta = df[column].diff()\n",
    "    gain = np.where(delta > 0, delta, 0)\n",
    "    loss = np.where(delta < 0, -delta, 0)\n",
    "    avg_gain = np.convolve(gain, np.ones(period)/period, mode='same')\n",
    "    avg_loss = np.convolve(loss, np.ones(period)/period, mode='same')\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(df, column=\"adjusted_close\", short_period=12, long_period=26, signal_period=9):\n",
    "    short_ema = df[column].ewm(span=short_period, adjust=False).mean()\n",
    "    long_ema = df[column].ewm(span=long_period, adjust=False).mean()\n",
    "    macd_line = short_ema - long_ema\n",
    "    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()\n",
    "    return macd_line, signal_line\n",
    "\n",
    "# Moving Averages: EMA 50 & EMA 200\n",
    "def compute_ema(df, column=\"adjusted_close\", period=50):\n",
    "    return df[column].ewm(span=period, adjust=False).mean()\n",
    "\n",
    "def compute_ema_200(df, column=\"adjusted_close\", period=200):\n",
    "    return df[column].ewm(span=period, adjust=False).mean()\n",
    "\n",
    "# Volatility Indicator: ATR\n",
    "def compute_atr(df, period=14):\n",
    "    tr = np.maximum(df[\"high\"] - df[\"low\"],\n",
    "                     np.maximum(abs(df[\"high\"] - df[\"adjusted_close\"].shift()),\n",
    "                                abs(df[\"low\"] - df[\"adjusted_close\"].shift())))\n",
    "    return tr.rolling(window=period).mean()\n",
    "\n",
    "# Trend Indicators: SAR, Slope & ADX\n",
    "def compute_slope(df, column=\"adjusted_close\", period=14):\n",
    "    return df[column].diff(period) / period\n",
    "\n",
    "def compute_sar(df, acceleration=0.02, maximum=0.2):\n",
    "    sar = np.zeros(len(df))\n",
    "    trend = 1  # Assume first trend is bullish\n",
    "    extreme_point = df[\"high\"].iloc[0]\n",
    "    af = acceleration\n",
    "    \n",
    "    for i in range(1, len(df)):\n",
    "        sar[i] = sar[i - 1] + af * (extreme_point - sar[i - 1]) if trend == 1 else sar[i - 1] - af * (sar[i - 1] - extreme_point)\n",
    "        extreme_point = max(extreme_point, df[\"high\"].iloc[i]) if trend == 1 else min(extreme_point, df[\"low\"].iloc[i])\n",
    "        af = min(af + acceleration, maximum)\n",
    "    return sar\n",
    "\n",
    "def compute_adx(df, period=14):\n",
    "    plus_dm = np.maximum(df[\"high\"].diff(), 0)\n",
    "    minus_dm = np.maximum(-df[\"low\"].diff(), 0)\n",
    "    tr = np.maximum(df[\"high\"] - df[\"low\"],\n",
    "                     np.maximum(abs(df[\"high\"] - df[\"adjusted_close\"].shift()),\n",
    "                                abs(df[\"low\"] - df[\"adjusted_close\"].shift())))\n",
    "    plus_di = 100 * (plus_dm.rolling(window=period).mean() / tr.rolling(window=period).mean())\n",
    "    minus_di = 100 * (minus_dm.rolling(window=period).mean() / tr.rolling(window=period).mean())\n",
    "    dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)\n",
    "    return dx.rolling(window=period).mean()\n",
    "\n",
    "# Volume Indicator: OBV\n",
    "def compute_obv_vectorized(df):\n",
    "    direction = np.sign(df[\"adjusted_close\"].diff()).fillna(0)\n",
    "    return (direction * df[\"volume\"]).cumsum()\n",
    "\n",
    "\n",
    "# Compute Indicators and Add to DataFrame\n",
    "df[\"EMA_50\"] = compute_ema(df, period=50)\n",
    "df[\"EMA_200\"] = compute_ema_200(df, period=200)\n",
    "df[\"RSI_14\"] = compute_rsi(df, period=14)\n",
    "df[\"MACD\"], df[\"MACD_Signal\"] = compute_macd(df)\n",
    "df[\"ATR_14\"] = compute_atr(df, period=14)\n",
    "df[\"SAR\"] = compute_sar(df)\n",
    "df[\"SLOPE_14\"] = compute_slope(df, period=14)\n",
    "df[\"ADX_14\"] = compute_adx(df, period=14)\n",
    "df[\"OBV\"] = compute_obv_vectorized(df)\n",
    "\n",
    "# Fill any NaNs from rolling calculations\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f6691-e7f9-4d49-88ba-c675a3ae9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a4296-f9b3-4ae5-9192-76a3c1ee50aa",
   "metadata": {},
   "source": [
    "### Regime Detection:\n",
    "Use Hidden Markov Models (HMMs) to identify market regimes (bull/bear) and create regime-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74894380-1c23-419b-ab35-7b101e839c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import mode\n",
    "\n",
    "# =========================\n",
    "# Regime Detection with HMM\n",
    "# =========================\n",
    "\n",
    "# --- Step 1: Define Number of Hidden States (Regimes) ---\n",
    "n_states = 3  # 3 regimes: Bullish, Bearish, Neutral\n",
    "\n",
    "# --- Step 2: Regime Detection with HMM ---\n",
    "# Selecting relevant features for HMM training\n",
    "hmm_features = [\"log_returns\", \"volatility_10\", \"RSI_14\", \"MACD\", \"ATR_14\", \"OBV\"]\n",
    "\n",
    "df_hmm = df[hmm_features].dropna()  # Avoid missing values\n",
    "X = df_hmm.values  \n",
    "\n",
    "# --- Step 3: Normalize Features for HMM ---\n",
    "scaler_hmm = StandardScaler()\n",
    "X_hmm_scaled = scaler_hmm.fit_transform(X)  # Standardize only HMM features\n",
    "\n",
    "# --- Step 4: Train HMM Model ---\n",
    "hmm_model = GaussianHMM(n_components=n_states, covariance_type=\"full\", n_iter=1000, random_state=42)\n",
    "hmm_model.fit(X_hmm_scaled)\n",
    "\n",
    "# --- Step 5: Predict Market Regimes ---\n",
    "df.loc[df_hmm.index, \"regime\"] = hmm_model.predict(X_hmm_scaled)\n",
    "\n",
    "# --- Step 6: Ensure all rows have a regime assigned ---\n",
    "df[\"regime\"] = df[\"regime\"].ffill().bfill()  \n",
    "\n",
    "# --- Step 7: Apply Smoothing to Reduce Noise in Regime Changes ---\n",
    "def rolling_mode(series, window=7):\n",
    "    def safe_mode(x):\n",
    "        m = mode(x, keepdims=True)\n",
    "        return m.mode[0] if len(m.mode) > 0 else x.iloc[-1]\n",
    "    # FIX: Use strictly forward-looking rolling (center=False)\n",
    "    return series.rolling(window, center=False).apply(safe_mode, raw=False)\n",
    "\n",
    "df[\"regime_smoothed\"] = rolling_mode(df[\"regime\"], window=7).ffill().bfill().astype(int)\n",
    "\n",
    "# --- Step 8: Mark Major Regime Change Points ---\n",
    "# FIX: Remove lookahead by not using shift(-15)\n",
    "df[\"regime_change\"] = df[\"regime_smoothed\"].diff().ne(0).astype(int)\n",
    "# (Optional alternative: If you prefer to confirm a change after 15 days, you could delay labeling:\n",
    "# df[\"regime_change\"] = df[\"regime_smoothed\"].diff().ne(0).shift(15).fillna(0).astype(int))\n",
    "\n",
    "# --- Step 9: Creating Regime-Based Features ---\n",
    "# To avoid mixing non-contiguous periods with the same regime label, we create a block id:\n",
    "df[\"block_id\"] = df[\"regime_smoothed\"].ne(df[\"regime_smoothed\"].shift()).cumsum()\n",
    "\n",
    "def regime_sma(df, column, window):\n",
    "    # Compute rolling average within each contiguous block only\n",
    "    return df.groupby(\"block_id\")[column].apply(lambda x: x.rolling(window, min_periods=1).mean()).reset_index(level=0, drop=True)\n",
    "\n",
    "# Apply Regime-Based Features\n",
    "df[\"SMA_50_regime\"] = regime_sma(df, \"adjusted_close\", 50)\n",
    "df[\"SMA_200_regime\"] = regime_sma(df, \"adjusted_close\", 200)\n",
    "df[\"volatility_10_regime\"] = regime_sma(df, \"volatility_10\", 10)\n",
    "df[\"RSI_14_regime\"] = regime_sma(df, \"RSI_14\", 14)\n",
    "\n",
    "df.dropna(inplace=True)  # Drop rows with missing values\n",
    "\n",
    "# --- Step 10: Fixing Regime Mapping and Visualization ---\n",
    "# Remap 0,1,2 → 1,2,3 for consistent labeling\n",
    "df[\"regime_smoothed\"] = df[\"regime_smoothed\"].map({0: 1, 1: 2, 2: 3})\n",
    "\n",
    "# Define color mapping and labels for visualization\n",
    "regime_colors = {1: \"red\", 2: \"green\", 3: \"blue\"}\n",
    "regime_labels = {1: \"Regime 1\", 2: \"Regime 2\", 3: \"Regime 3\"}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot regime-based adjusted close prices\n",
    "for regime, color in regime_colors.items():\n",
    "    plt.plot(df[df[\"regime_smoothed\"] == regime].index,\n",
    "             df[df[\"regime_smoothed\"] == regime][\"adjusted_close\"],\n",
    "             linestyle='-', marker='.', label=regime_labels[regime], color=color)\n",
    "# Highlight regime change points\n",
    "for change_date in df[df[\"regime_change\"] == 1].index:\n",
    "    plt.axvline(change_date, color='black', linestyle=\"--\", alpha=0.5)\n",
    "plt.title(\"Market Regimes Detected by HMM\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Adjusted Close Price\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1_regime_detection_chart.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Step 11: Fixed Regime Frequency Distribution ---\n",
    "plt.figure(figsize=(6, 4))\n",
    "regime_counts = df[\"regime_smoothed\"].value_counts().sort_index()\n",
    "bars = plt.bar(regime_counts.index, regime_counts.values, color=[regime_colors[i] for i in regime_counts.index])\n",
    "plt.xticks(ticks=regime_counts.index, labels=regime_counts.index)\n",
    "plt.title(\"Regime Frequency Distribution\")\n",
    "plt.xlabel(\"Regime\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2_regime_frequency_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f1dbb-9955-445b-bb07-89b1e1961a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Ensure \"regime_smoothed\" Exists  ---\n",
    "if \"regime_smoothed\" not in df.columns:\n",
    "    print(\"Warning: 'regime_smoothed' column is missing! Recreating it...\")\n",
    "    if \"regime\" in df.columns:\n",
    "        df[\"regime_smoothed\"] = df[\"regime\"].astype(int)\n",
    "    else:\n",
    "        raise KeyError(\"Error: Neither 'regime_smoothed' nor 'regime' found in DataFrame!\")\n",
    "\n",
    "# --- Step 2: Drop Any Old One-Hot Encoded Regime Columns (But NOT \"regime_smoothed\") ---\n",
    "old_regime_cols = [col for col in df.columns if col.startswith(\"regime_\") and col != \"regime_smoothed\"]\n",
    "if old_regime_cols:\n",
    "    df.drop(columns=old_regime_cols, inplace=True, errors=\"ignore\")\n",
    "    \n",
    "# Preserve \"regime_smoothed\" before encoding to prevent KeyErrors\n",
    "df[\"regime_smoothed_backup\"] = df[\"regime_smoothed\"]\n",
    "\n",
    "# --- Step 3: One-Hot Encode Regimes for LSTM Training ---\n",
    "df = pd.get_dummies(df, columns=[\"regime_smoothed\"], prefix=\"regime\")\n",
    "\n",
    "# Ensure All Expected Regime Columns Exist (Fixes Missing regime_1)\n",
    "expected_regimes = [\"regime_1\", \"regime_2\", \"regime_3\"]\n",
    "for col in expected_regimes:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0  # Add missing regime column and fill with 0s\n",
    "\n",
    "if \"regime_smoothed\" not in df.columns:\n",
    "    df[\"regime_smoothed\"] = df[\"regime_smoothed_backup\"]\n",
    "    \n",
    "df.drop(columns=[\"regime_smoothed_backup\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# --- Step 4: Convert One-Hot Encoded Values to Integers ---\n",
    "df[expected_regimes] = df[expected_regimes].astype(int)\n",
    "\n",
    "# --- Step 5: Verify the Fix ---\n",
    "display(df.tail())  # Check if columns are correct\n",
    "\n",
    "print(\"Regime counts in dataset:\")\n",
    "print(df[expected_regimes].sum())  # Count number of rows for each regime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aed7d2-7a6e-40c2-80af-e989ca8bd794",
   "metadata": {},
   "source": [
    "### Adding Lag Features (`lag_1` to `lag_7`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1438730-18c9-4650-b7ce-59a89982de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lag features (up to 7 days)\n",
    "lags = 7\n",
    "for lag in range(1, lags + 1):\n",
    "    df[f'lag_{lag}'] = df['log_returns'].shift(lag)\n",
    "\n",
    "# Drop NaN values caused by shifting\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Check DataFrame structure after adding lag features \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ac2f5-3a00-4202-b166-7ffb8ec07b9e",
   "metadata": {},
   "source": [
    "### Feature Correlation Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073f9f0-69ab-47df-8a30-2d39dfee5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Generate a mask to hide the upper triangle (reduce redundancy)\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Dynamic figure sizing based on the number of features\n",
    "n_features = corr_matrix.shape[0]\n",
    "plt.figure(figsize=(n_features * 0.5, n_features * 0.5))  # Adjust multiplier if needed\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix, mask=mask, annot=False, fmt=\".2f\", cmap=\"coolwarm\", center=0,\n",
    "    linewidths=1, cbar_kws={\"shrink\": 0.75}, square=True\n",
    ")\n",
    "\n",
    "# Label formatting\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(\"Feature Correlation Matrix\", fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Save figure (Change filename & format as needed)\n",
    "plt.savefig(\"3_feature_correlation_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf0bd2-d156-4c85-85a7-5b9f8d6561b5",
   "metadata": {},
   "source": [
    "### Definition of Target Variable (`d`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78fc21-7a85-4c48-87d1-ec65022cb4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Target Variable (`d`) ---\n",
    "df['d'] = np.where(df['log_returns'].shift(-1) > 0, 1, 0)\n",
    "\n",
    "# Drop only NaN values in 'd' caused by shifting (Last row)\n",
    "df.dropna(subset=['d'], inplace=True)\n",
    "\n",
    "# --- Corrected Feature Selection (X) ---\n",
    "X = df[[col for col in [\n",
    "    'open', 'high', 'low', 'close', 'adjusted_close', 'volume', 'log_returns', \n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', \n",
    "    'volatility_10', 'vol_adj_returns', 'EMA_50', 'EMA_200',\n",
    "    'RSI_14', 'MACD', 'MACD_Signal', 'ATR_14', 'SAR',\n",
    "    'SLOPE_14', 'ADX_14', 'OBV', 'SMA_50_regime', 'SMA_200_regime', \n",
    "    'volatility_10_regime', 'RSI_14_regime', 'regime', 'regime_1', 'regime_2', 'regime_3'\n",
    "] if col in df.columns]]  # Only include columns that exist\n",
    "\n",
    "# --- Define Target Variable (y) ---\n",
    "y = df['d']\n",
    "\n",
    "# Verify Structure\n",
    "print(\"Feature Matrix (X) Shape:\", X.shape)\n",
    "print(\"Target Variable (y) Shape:\", y.shape)\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978157e-f6cd-44b0-8233-95acf93903f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['d'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12dcb4f-9ca7-4e7d-a016-b103e2482032",
   "metadata": {},
   "source": [
    "### Feature Selection Using `ExtraTreesClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b885f14-9d03-4fa8-a4e1-2a65aa82d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) Define a chronological cutoff date\n",
    "train_cutoff = '2022-01-01'  # or pick another date\n",
    "\n",
    "df_train = df.loc[:train_cutoff]\n",
    "df_test = df.loc[train_cutoff:]\n",
    "\n",
    "X_train = X.loc[df_train.index]\n",
    "y_train = y.loc[df_train.index]\n",
    "X_test = X.loc[df_test.index]\n",
    "y_test = y.loc[df_test.index]\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# 2) Fit ExtraTrees ONLY on the training set\n",
    "model_fs = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "model_fs.fit(X_train, y_train)\n",
    "\n",
    "# 3) Extract feature importances from TRAINING\n",
    "feature_importance = pd.Series(\n",
    "    model_fs.feature_importances_,\n",
    "    index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# (Optional) plot these feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=feature_importance.values, \n",
    "    y=feature_importance.index, \n",
    "    hue=feature_importance.index,\n",
    "    palette=\"viridis\",\n",
    "    dodge=False,\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Feature Importance (ExtraTreesClassifier) - Training Set\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"4_feature_importance_extratrees.png\", dpi=100)  # or .pdf/.svg if needed\n",
    "plt.show()\n",
    "\n",
    "# 4) Select top N features from TRAINING\n",
    "n_top = 15\n",
    "top_features = feature_importance.index[:n_top]\n",
    "print(f\"Top {n_top} features (Training Set):\", top_features.tolist())\n",
    "\n",
    "# 5) Apply these features to both train & test\n",
    "X_train_selected = X_train[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "print(\"X_train_selected shape:\", X_train_selected.shape)\n",
    "print(\"X_test_selected shape:\", X_test_selected.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be687a-498d-4ba1-9493-8e75004988b2",
   "metadata": {},
   "source": [
    "### Preparation of `X` and `y` for LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318cf38-b043-4e7e-a5ff-ab6eb6452e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Step 2: Scale Training Only\n",
    "##############################\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)            # Fit on training\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform test with same scaler\n",
    "\n",
    "# Convert back to DataFrame for convenience\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "##############################\n",
    "# Step 3: Use Top Features\n",
    "##############################\n",
    "# 'top_features' is assumed to be known (selected from a training-only feature selection step)\n",
    "# e.g., top_features = ['RSI_14', 'lag_6', 'lag_3', 'lag_5', 'vol_adj_returns', ...]\n",
    "X_train_selected = X_train_scaled[top_features]\n",
    "X_test_selected = X_test_scaled[top_features]\n",
    "\n",
    "# Align target with these new DataFrame indices\n",
    "y_train = y_train.loc[X_train_selected.index]\n",
    "y_test = y_test.loc[X_test_selected.index]\n",
    "\n",
    "print(\"X_train_selected shape:\", X_train_selected.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test_selected shape:\", X_test_selected.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "##############################\n",
    "# Step 4: Sequence Creation\n",
    "##############################\n",
    "def create_sequences(X_df, y_series, seq_length=7):\n",
    "    \"\"\"\n",
    "    Generates sequences from a DataFrame (X_df) and aligns with the target (y_series).\n",
    "    Returns (X_seq, y_seq).\n",
    "    \"\"\"\n",
    "    X_values = X_df.values  \n",
    "    y_values = y_series.values  \n",
    "    \n",
    "    # For each valid starting index i, the sequence is [i : i + seq_length]\n",
    "    X_seq = np.array([\n",
    "        X_values[i : i + seq_length] \n",
    "        for i in range(len(X_values) - seq_length)\n",
    "    ])\n",
    "    # The label for that sequence is y at index i+seq_length\n",
    "    y_seq = y_values[seq_length:]  \n",
    "    \n",
    "    return X_seq, y_seq\n",
    "\n",
    "sequence_length = 7\n",
    "\n",
    "# Safety checks\n",
    "if len(X_train_selected) < sequence_length:\n",
    "    raise ValueError(\"Not enough training data to generate sequences! Adjust sequence_length or dataset.\")\n",
    "if len(X_test_selected) < sequence_length:\n",
    "    raise ValueError(\"Not enough test data to generate sequences! Adjust sequence_length or dataset.\")\n",
    "\n",
    "# Generate training sequences\n",
    "X_seq_train, y_seq_train = create_sequences(X_train_selected, y_train, seq_length=sequence_length)\n",
    "# Generate test sequences\n",
    "X_seq_test, y_seq_test = create_sequences(X_test_selected, y_test, seq_length=sequence_length)\n",
    "\n",
    "##############################\n",
    "# Step 5: Confirm Shapes\n",
    "##############################\n",
    "print(\"\\nSequence Shapes:\")\n",
    "print(f\"X_seq_train: {X_seq_train.shape} (Samples={X_seq_train.shape[0]}, Time Steps={sequence_length}, Features={X_seq_train.shape[2]})\")\n",
    "print(f\"y_seq_train: {y_seq_train.shape} (Samples={y_seq_train.shape[0]})\")\n",
    "print(f\"X_seq_test:  {X_seq_test.shape} (Samples={X_seq_test.shape[0]}, Time Steps={sequence_length}, Features={X_seq_test.shape[2]})\")\n",
    "print(f\"y_seq_test:  {y_seq_test.shape} (Samples={y_seq_test.shape[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0aa8f-9e5b-4057-8d15-4998def92370",
   "metadata": {},
   "source": [
    "# Model Implementation & Training\n",
    "## Model Definition & Initialization\n",
    "\n",
    "#### LSTM Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e051f7-64be-40e9-bc23-58dec5902d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Model Definition & Initialization (uses CONFIG BLOCK values)\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(SEQUENCE_LENGTH, N_FEATURES)),  # Explicit Input layer\n",
    "\n",
    "    # First LSTM layer: returns sequences for the next LSTM layer\n",
    "    LSTM(LSTM_UNITS_1, return_sequences=True),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "\n",
    "    # Second LSTM layer: processes the sequence and returns the final output\n",
    "    LSTM(LSTM_UNITS_2, return_sequences=False),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "\n",
    "    # Dense layer for binary classification\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early Stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd4f1d-348b-4e81-a21e-9da78cb6e4e8",
   "metadata": {},
   "source": [
    "## Model Training with Rolling Cross-Validation\n",
    "### Training Implementation\n",
    "\n",
    "* Rolling Time-Series Cross-Validation → Ensures robust evaluation.\n",
    "* Scaling within each fold → Prevents data leakage.\n",
    "* Early Stopping → Stops training if validation loss stops improving.\n",
    "* Standard Metrics → Accuracy, Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd4aed-387c-461c-b424-2e41863b279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====================================================\n",
    "# Function that pulls config params from the top block\n",
    "# ====================================================\n",
    "def get_stacked_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(LSTM_UNITS_1, return_sequences=True),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        LSTM(LSTM_UNITS_2, return_sequences=False),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ====================================================\n",
    "# Time-Series Cross-Validation Setup\n",
    "# ====================================================\n",
    "tscv = TimeSeriesSplit(n_splits=6, test_size=252)\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_seq_train)):\n",
    "    print(f\"\\nProcessing Fold {fold + 1}/{tscv.n_splits}...\")\n",
    "\n",
    "    # Extract fold data\n",
    "    X_train_fold = X_seq_train[train_index]\n",
    "    X_test_fold = X_seq_train[test_index]\n",
    "    y_train_fold = y_seq_train[train_index]\n",
    "    y_test_fold = y_seq_train[test_index]\n",
    "\n",
    "    # Scale fold data (3D ➝ 2D ➝ 3D)\n",
    "    nsamples_train, timesteps, n_feats = X_train_fold.shape\n",
    "    X_train_fold_2d = X_train_fold.reshape(-1, n_feats)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold_scaled = scaler.fit_transform(X_train_fold_2d).reshape(X_train_fold.shape)\n",
    "    X_test_fold_scaled = scaler.transform(X_test_fold.reshape(-1, n_feats)).reshape(X_test_fold.shape)\n",
    "\n",
    "    # Build and train model\n",
    "    model = get_stacked_model((timesteps, n_feats))\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fold_scaled, y_train_fold,\n",
    "        validation_data=(X_test_fold_scaled, y_test_fold),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Predict and store results\n",
    "    pred = model.predict(X_test_fold_scaled).flatten()\n",
    "    pred_binary = [1 if p >= 0.5 else 0 for p in pred]\n",
    "\n",
    "    predictions.extend(pred)\n",
    "    true_labels.extend(y_test_fold)\n",
    "\n",
    "    # Compute and store fold metrics\n",
    "    fold_metrics.append({\n",
    "        \"fold\": fold + 1,\n",
    "        \"accuracy\": accuracy_score(y_test_fold, pred_binary),\n",
    "        \"precision\": precision_score(y_test_fold, pred_binary),\n",
    "        \"recall\": recall_score(y_test_fold, pred_binary),\n",
    "        \"f1_score\": f1_score(y_test_fold, pred_binary)\n",
    "    })\n",
    "\n",
    "# ====================================================\n",
    "# Save & Display Aggregate Metrics\n",
    "# ====================================================\n",
    "df_fold_metrics = pd.DataFrame(fold_metrics)\n",
    "df_fold_metrics.to_csv(\"fold_metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nFold Metrics:\")\n",
    "display(df_fold_metrics)\n",
    "\n",
    "# Overall performance\n",
    "predictions_binary = [1 if p >= 0.5 else 0 for p in predictions]\n",
    "print(\"\\nCross-Validated Model Performance (Training Set):\")\n",
    "print(f\"Accuracy: {accuracy_score(true_labels, predictions_binary):.4f}\")\n",
    "print(f\"Precision: {precision_score(true_labels, predictions_binary):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_labels, predictions_binary):.4f}\")\n",
    "print(f\"F1-score: {f1_score(true_labels, predictions_binary):.4f}\")\n",
    "\n",
    "print(\"\\nMean Fold Performance:\")\n",
    "print(df_fold_metrics[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c0b1a-4808-46af-b5cd-cb60cfb63579",
   "metadata": {},
   "source": [
    "#### `Saving & Logging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa82821b-6db2-437e-b7a5-5f2092e909b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil \n",
    "\n",
    "# Save the best model from cross-validation\n",
    "model.save(\"best_lstm_model.keras\")\n",
    "\n",
    "# Backup in case of overwriting\n",
    "shutil.copy(\"best_lstm_model.keras\", \"best_lstm_model_backup.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb71dc3-180d-4009-ac16-2c2e092ce6d2",
   "metadata": {},
   "source": [
    "**Up to this section, we have put together a production-grade pipeline for an LSTM-based trading strategy.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cb0fc-e2a7-4288-8d7b-e8092dbc370d",
   "metadata": {},
   "source": [
    "# Trading Signal Generation and Backtesting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2ca88-5a41-40a4-b1d2-ce0d9ff0e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backtest(\n",
    "    df, predictions, \n",
    "    transaction_cost=0.0001, \n",
    "    long_threshold=0.505, \n",
    "    short_threshold=0.495, \n",
    "    min_hold_days=3,\n",
    "    stop_loss_threshold=-0.025,  # Stop-loss: Exit if daily loss > -2.5%\n",
    "    rolling_loss_threshold=-0.05,  # Exit if rolling 10-day return < -5%\n",
    "    volatility_scaling=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Backtests a trading strategy with:\n",
    "    - Lowered probability thresholds for better trade execution.\n",
    "    - Adaptive holding period (exit when probability confidence weakens).\n",
    "    - Stop-loss mechanism to prevent major drawdowns.\n",
    "    - Volatility-based position scaling.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Historical price data including 'log_returns'.\n",
    "        predictions (array-like): Model output probabilities for the test period.\n",
    "        transaction_cost (float): Cost per trade.\n",
    "        long_threshold (float): Minimum probability to enter a long trade.\n",
    "        short_threshold (float): Maximum probability to enter a short trade.\n",
    "        min_hold_days (int): Minimum days to hold a position once taken.\n",
    "        stop_loss_threshold (float): Stop-loss exit threshold per trade.\n",
    "        rolling_loss_threshold (float): Exit if rolling 10-day loss > threshold.\n",
    "        volatility_scaling (bool): If True, position size scales based on volatility.\n",
    "\n",
    "    Returns:\n",
    "        df_backtest (DataFrame): DataFrame with backtest results including an equity curve.\n",
    "    \"\"\"\n",
    "    df_backtest = df.copy()\n",
    "    \n",
    "    # Align predictions with the last len(predictions) rows of df_backtest\n",
    "    df_backtest['p'] = np.nan\n",
    "    df_backtest.loc[df_backtest.index[-len(predictions):], 'p'] = predictions\n",
    "\n",
    "    # Generate raw trading signals based on adjusted thresholds\n",
    "    df_backtest['raw_signal'] = np.select(\n",
    "        [df_backtest['p'] >= long_threshold, df_backtest['p'] <= short_threshold],\n",
    "        [1, -1], default=0\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # Adaptive Holding Period\n",
    "    # ---------------------------\n",
    "    raw_signals = df_backtest['raw_signal']\n",
    "    final_positions = []\n",
    "    current_position = 0\n",
    "    hold_counter = 0\n",
    "\n",
    "    for i in range(len(raw_signals)):\n",
    "        if hold_counter > 0:\n",
    "            # Continue holding the position\n",
    "            final_positions.append(current_position)\n",
    "            hold_counter -= 1\n",
    "        else:\n",
    "            # Allow new signal entry\n",
    "            if raw_signals.iloc[i] != 0:\n",
    "                current_position = raw_signals.iloc[i]\n",
    "                hold_counter = min_hold_days - 1  # Hold for at least min_hold_days\n",
    "                final_positions.append(current_position)\n",
    "            else:\n",
    "                # Exit if probability confidence weakens\n",
    "                current_position = 0\n",
    "                final_positions.append(0)\n",
    "\n",
    "    df_backtest['po'] = final_positions\n",
    "    df_backtest['po_shifted'] = df_backtest['po'].shift(1).fillna(0)  # Avoid look-ahead bias\n",
    "\n",
    "    # ---------------------------\n",
    "    # Stop-Loss Implementation\n",
    "    # ---------------------------\n",
    "    df_backtest['daily_return'] = df_backtest['po_shifted'] * df_backtest['log_returns']\n",
    "    \n",
    "    # Apply stop-loss: Exit position if loss exceeds stop_loss_threshold\n",
    "    stop_loss_exit = df_backtest['daily_return'] <= stop_loss_threshold\n",
    "    df_backtest.loc[stop_loss_exit, 'po_shifted'] = 0\n",
    "\n",
    "    # Apply rolling 10-day loss exit condition\n",
    "    df_backtest['rolling_loss'] = df_backtest['daily_return'].rolling(10).sum()\n",
    "    df_backtest.loc[df_backtest['rolling_loss'] <= rolling_loss_threshold, 'po_shifted'] = 0\n",
    "\n",
    "    # ---------------------------\n",
    "    # Volatility-Based Position Sizing\n",
    "    # ---------------------------\n",
    "    if volatility_scaling:\n",
    "        df_backtest['volatility_30d'] = df_backtest['log_returns'].rolling(30).std()\n",
    "        avg_volatility = df_backtest['volatility_30d'].mean()\n",
    "        df_backtest['position_size'] = df_backtest['po_shifted'] * (avg_volatility / df_backtest['volatility_30d'])\n",
    "    else:\n",
    "        df_backtest['position_size'] = df_backtest['po_shifted']\n",
    "\n",
    "    # ---------------------------\n",
    "    # Compute Strategy Returns\n",
    "    # ---------------------------\n",
    "    df_backtest['strategy_log_return'] = df_backtest['position_size'] * df_backtest['log_returns']\n",
    "\n",
    "    # ---------------------------\n",
    "    # Apply Transaction Costs\n",
    "    # ---------------------------\n",
    "    df_backtest['trade_cost'] = transaction_cost * df_backtest['po'].diff().abs().fillna(0)\n",
    "    df_backtest['strategy_net_log_return'] = df_backtest['strategy_log_return'] - df_backtest['trade_cost']\n",
    "\n",
    "    # ---------------------------\n",
    "    # Compute Equity Curve\n",
    "    # ---------------------------\n",
    "    df_backtest['equity_curve'] = np.exp(df_backtest['strategy_net_log_return'].cumsum())\n",
    "\n",
    "    return df_backtest\n",
    "\n",
    "#  Run the backtest\n",
    "df_backtest = run_backtest(df, predictions)\n",
    "\n",
    "#  Print results\n",
    "print(df_backtest[['log_returns', 'p', 'po', 'strategy_log_return', 'trade_cost', 'strategy_net_log_return', 'equity_curve']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3e6c3-1b8e-45ae-9c1f-fd4ef5b05627",
   "metadata": {},
   "source": [
    "# AI Strategy Performance Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d61f8f-7823-4f6d-a52a-1a97508da3a0",
   "metadata": {},
   "source": [
    "## Plot 1: Actual Returns vs Predicted Returns\n",
    "Shows how well the signals align with actual movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c951b-cbb5-4d34-9261-cd79706750f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_backtest.index, df_backtest['log_returns'], label=\"Actual Returns\", alpha=0.45)\n",
    "plt.plot(df_backtest.index, df_backtest['strategy_log_return'], label=\"Predicted Returns\", alpha=0.45)\n",
    "plt.axhline(y=0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.title(\"Actual Returns vs Predicted Returns\")\n",
    "plt.ylabel(\"Daily Log Returns\")\n",
    "plt.legend()\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"5_actual_vs_predicted_returns.png\", dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d54134-b172-47f2-9b97-aae40af4648a",
   "metadata": {},
   "source": [
    "## Plot 2: Equity Curve of the Strategy\n",
    "Show the cumulative performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ab33d-de6a-4b2d-8ce1-2a3fad06cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_backtest.index, df_backtest['equity_curve'], label=\"Strategy Equity Curve\", color='blue')\n",
    "plt.axhline(1.0, linestyle='--', color='red', label=\"Baseline\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Returns\")\n",
    "plt.title(\"Trading Strategy Backtest: Equity Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"6_equity_curve_strategy.png\", dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3046c9-c1f8-48a5-a81c-2556258dd714",
   "metadata": {},
   "source": [
    "## Plot 3: Strategy vs. Benchmark Comparison\n",
    "Illustrate whether the strategy outperforms a simple buy-and-hold benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16df42-db06-4610-b0b9-6e6f4f575d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert log returns to cumulative returns for buy-and-hold:\n",
    "df_backtest['buy_and_hold'] = np.exp(df_backtest['log_returns'].cumsum())\n",
    "df_backtest['cumulative_strategy'] = df_backtest['equity_curve']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_backtest.index, df_backtest['buy_and_hold'], label=\"Buy-and-Hold (Benchmark)\", alpha=0.55)\n",
    "plt.plot(df_backtest.index, df_backtest['cumulative_strategy'], label=\"LSTM Strategy\", alpha=0.55)\n",
    "plt.axhline(y=1, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.title(\"Cumulative Performance: Strategy vs. Benchmark\")\n",
    "plt.ylabel(\"Cumulative Returns\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"7_cumulative_performance_vs_benchmark.png\", dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099b010-0e33-419d-8613-2cbf3e112669",
   "metadata": {},
   "source": [
    "## `quantstats` Analytics Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6631e-c0f7-411a-b720-0f05d3258080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert net log returns to simple daily returns:\n",
    "strategy_simple_returns = np.exp(df_backtest['strategy_net_log_return']) - 1\n",
    "benchmark_simple_returns = np.exp(df_backtest['log_returns']) - 1\n",
    "\n",
    "import quantstats as qs\n",
    "\n",
    "# Generate a Performance Report\n",
    "qs.reports.basic(strategy_simple_returns, benchmark=benchmark_simple_returns)\n",
    "\n",
    "# Cumulative Returns Comparison (Strategy vs. Benchmark)\n",
    "qs.plots.returns(strategy_simple_returns, benchmark=benchmark_simple_returns)\n",
    "\n",
    "# Drawdown Analysis\n",
    "qs.plots.drawdown(strategy_simple_returns)\n",
    "\n",
    "# Rolling Sharpe Ratio\n",
    "qs.plots.rolling_sharpe(strategy_simple_returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2fe6b-b360-4316-baed-fcb7e06975d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export Clean LSTM Model Results (Fixed) ===\n",
    "\n",
    "df_lstm = df_backtest[['log_returns', 'p']].copy()\n",
    "df_lstm.rename(columns={'p': 'predictions'}, inplace=True)\n",
    "\n",
    "# Drop rows with missing predictions (common in time series)\n",
    "df_lstm.dropna(subset=['predictions'], inplace=True)\n",
    "\n",
    "# Confirm structure\n",
    "print(df_lstm.head())\n",
    "print(df_lstm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff857c9-0e56-401a-a2a6-651d7aae7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm.to_csv('df_lstm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
