{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cced5bb-0605-4a3f-bf77-50efbaec300f",
   "metadata": {},
   "source": [
    "# 04 Att Lstm Model Attention Weights\n",
    "> ⚠️ This notebook supports the attention-based hybrid strategy  \n",
    "> (Notebook `08_Hybrid_Attention_Allocation.ipynb`) by generating and exporting \n",
    "> attention weights and processed input sequences.\n",
    "\n",
    "This notebook builds and runs the Attention-LSTM model, then extracts **real attention weights** and aligns them with actual prediction dates for interpretability and integration into hybrid strategy logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fff157-2d5e-400c-9427-2d336fae12d3",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data Used to Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f00a4f-b322-432b-ba67-69cee34f1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75882a6-5bf0-45cb-936f-e6b16d8b4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- API Configuration ---\n",
    "API_KEY = os.getenv(\"EODHD_API_KEY\")  # Load API Key securely\n",
    "TICKER = \"GSPC.INDX\"\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2025-01-01\"\n",
    "BASE_URL = \"https://eodhd.com/api/eod/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ff31c-7e3d-4e07-800a-a4c0cfaa9110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_eod_data(ticker, api_key, start_date, end_date, retries=3, delay=5):\n",
    "    \"\"\"\n",
    "    Fetches historical market data from EODHD API with retry logic.\n",
    "\n",
    "    Parameters:\n",
    "    - ticker (str): Stock or index ticker symbol\n",
    "    - api_key (str): API authentication token\n",
    "    - start_date (str): Start date for data retrieval\n",
    "    - end_date (str): End date for data retrieval\n",
    "    - retries (int): Number of retry attempts in case of failure\n",
    "    - delay (int): Delay between retries (exponential backoff)\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Market data if successful, else None\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}{ticker}?api_token={api_key}&from={start_date}&to={end_date}&fmt=json\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data:\n",
    "                    logging.info(f\"Successfully retrieved {len(data)} records.\")\n",
    "                    return pd.DataFrame(data)\n",
    "                else:\n",
    "                    logging.warning(\"API returned an empty dataset.\")\n",
    "            else:\n",
    "                logging.error(f\"API request failed with status {response.status_code}: {response.text}\")\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"API request error: {e}\")\n",
    "\n",
    "        sleep(delay * (2 ** attempt))  # Exponential backoff\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95e9f0-88c8-4679-9cfa-10e3d0a0daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_eod_data(TICKER, API_KEY, START_DATE, END_DATE)\n",
    "\n",
    "if df is not None:\n",
    "    \n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    df.set_index(\"date\", inplace=True)\n",
    "\n",
    "    numeric_cols = [\"open\", \"high\", \"low\", \"close\", \"adjusted_close\", \"volume\"]\n",
    "    df[numeric_cols] = df[numeric_cols].astype(float)\n",
    "\n",
    "    df.ffill(inplace=True)  \n",
    "\n",
    "    display(df.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b047ecf-50d7-4d42-9165-8af374d6ad7c",
   "metadata": {},
   "source": [
    "## 2. Load Saved Sequences & Test Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becaa9f6-affb-4547-810a-9cbb7e6c486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load saved numpy sequences and test dates\n",
    "X_seq_test = np.load(\"../data/X_seq_test.npy\")\n",
    "# Assuming test_index.npy contains datetime-formatted index\n",
    "test_index = pd.to_datetime(np.load(\"../data/test_index.npy\"))\n",
    "\n",
    "print(\"Test Sequence Shape:\", X_seq_test.shape)\n",
    "print(\"Sample Dates:\", test_index[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6062ba-c921-4230-a424-602dc407b695",
   "metadata": {},
   "source": [
    "## 3. Rebuild Attention-LSTM Model (Same as Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f7715-3eca-4ad5-b9d6-aac2f809e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Multiply\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# Define Attention Layer\n",
    "def attention_layer(inputs):\n",
    "    attention_scores = Dense(1, activation='tanh', name=\"attention_scores\")(inputs)\n",
    "    attention_scores = tf.squeeze(attention_scores, axis=-1)\n",
    "    attention_weights = tf.nn.softmax(attention_scores, axis=1, name=\"attention_weights\")\n",
    "    context_vector = Multiply(name=\"weighted_sum\")([inputs, tf.expand_dims(attention_weights, axis=-1)])\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1, name=\"context_vector\")\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "# Model Function\n",
    "def get_attention_model(input_shape, lstm_units=80, dropout_rate=0.1, lr=2.6e-5):\n",
    "    inputs = Input(shape=input_shape, name=\"input_layer\")\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True, name=\"lstm_layer\")(inputs)\n",
    "    lstm_out = Dropout(dropout_rate, name=\"dropout_layer\")(lstm_out)\n",
    "    context_vector, attention_weights = attention_layer(lstm_out)\n",
    "    output = Dense(1, activation='sigmoid', name=\"output_layer\")(context_vector)\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Rebuild Model\n",
    "input_shape = (X_seq_test.shape[1], X_seq_test.shape[2])\n",
    "attention_model = get_attention_model(input_shape)\n",
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71e0d7-a80b-41e6-92c6-29a337b70cc3",
   "metadata": {},
   "source": [
    "## 4. Load Trained Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af782e-63ea-4c33-80fe-d2f600222605",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model.save_weights(\"../models/att_lstm_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c080cc-3c3a-46f9-8405-1a25339ac436",
   "metadata": {},
   "source": [
    "## 5. Predict and Extract Attention Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7585f-3813-43bd-b1cf-5a5c0d8e7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = attention_model.predict(X_seq_test, verbose=0)\n",
    "\n",
    "# Access attention weights properly from internal tensor\n",
    "extract_attention = Model(inputs=attention_model.input,\n",
    "                          outputs=attention_model.get_layer(\"dropout_layer\").output)\n",
    "\n",
    "lstm_output = extract_attention.predict(X_seq_test)\n",
    "\n",
    "# Now reapply attention to this LSTM output\n",
    "attention_scores = tf.squeeze(Dense(1, activation='tanh')(lstm_output), axis=-1)\n",
    "attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "\n",
    "# Collapse across time\n",
    "attention_mean = attention_weights.numpy().mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d878be-daaf-4ced-80f8-8f2cc32f01d7",
   "metadata": {},
   "source": [
    "## 6. Align and Save Attention Scores DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642a5ce-fe46-4358-9d89-e2197618f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align index to match predictions/attention size\n",
    "att_df = pd.DataFrame({\n",
    "    'predictions': predictions.flatten(),\n",
    "    'attention_mean': attention_mean\n",
    "}, index=test_index[-len(predictions):])  # Use last 746 dates\n",
    "\n",
    "# Save\n",
    "att_df.to_csv(\"../data/df_att_with_attention.csv\")\n",
    "att_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
